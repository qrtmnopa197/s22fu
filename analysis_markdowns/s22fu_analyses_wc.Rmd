---
title: "Full analysis"
author: "Daniel Parr"
knit: (function(inputFile, encoding) {
  rmarkdown::render(inputFile, encoding = encoding, output_dir = "/Users/dp/projects/s22_follow_up/data_review_analysis_records") })
output: html_document
---
# Overview
This markdown contains analyses done on the full s22fu dataset.

# Setting up
Loading in functions and data; setting paths and options.
```{r 1, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(mc.cores=12)

##SET MANUALLY
path_to_project_directory <- "~/projects/s22_follow_up/"
path_to_s22 <- "~/projects/spring_2022_study/"
##############
stan_model_dir_s22fu <- paste0(path_to_project_directory,"code/stan_models/")
stan_model_dir_s22 <- paste0(path_to_s22,"code/stan_models/final_models/")
model_out_dir <- paste0(path_to_project_directory,"output/results/stan_model_fits/")

library(cmdstanr)
library(tidyverse)
library(tidybayes)
library(loo)
library(GGally)
library(bayesplot)
library(sigmoid)
library(abind)
library(lmerTest)

source(paste0(path_to_project_directory,"code/functions/s22fu_utilities.R"))
source(paste0(path_to_s22,"code/functions/s22_utilities.R"))
source(paste0(path_to_s22,"code/functions/stan_utilities.R"))
source(paste0(path_to_s22,"code/functions/fit_stan_model.R"))

trials <- read.csv(paste0(path_to_project_directory,"analysis_data/trial_level_data_all_subs_2023-06-12_09_56_03.csv"))
subs <- read.csv(paste0(path_to_project_directory,"analysis_data/sub_level_data_all_subs_2023-06-12_09_56_03.csv"))
```

# Pre-processing
Eliminate subject who fail to meet quality standards

```{r 2}
#identify subjects who fail the hard QC cutoffs
sub_hard_fail <- subs %>% filter(att_checks_passed < 3 |
                            percent_left > .8 |
                            percent_left < .2 |
                            consecutive_late_choices > 5 |
                            late_percent > .2 |
                            answers_incorrect > 2 |
                            trials_completed < 104 |
                            valrate_skipped_percent > .14 |
                            valence_sd < .1 |
                            decrate_sd < .05 |
                            feedrate_sd < .05)

#count the number of soft QC cutoffs each subject meets
subs$softs <- 0
for(i in 1:nrow(subs)){
  subs$softs[i] <- length(which(c(subs$consecutive_auto_process[i] > 4,subs$att_checks_passed[i] < 4,subs$answers_incorrect[i] > 1,
                                  subs$late_percent[i] > .1,subs$valrate_skipped_percent[i] > .07,
                                  subs$choice_pt_completed[i] == 0, subs$decrate_pt_completed[i] == 0, subs$feedrate_pt_completed == 0)))
}
sub_soft_fail <- filter(subs,softs >= 2) #identify those who meet more than 2 soft cutoffs

subs_to_exclude <- unique(c(sub_hard_fail$id,sub_soft_fail$id)) #mark subjects who fail on either hard or soft or criteria for exclusion

trials <- trials %>% filter(!(id %in% subs_to_exclude)) %>% filter(choice != "late") #filter out bad subjects, as well as trials on which the subject failed to make a choice
subs <- subs %>% filter(!(id %in% subs_to_exclude))
```

Do a few data transformations.
```{r 3}
trials <- add_sub_indices(trials) #add subject indices to the df. These will match the indices used in Stan.

#add a column with completed trial numbers - the trial indices if you ignore late trials. These will match the "t" trial numbers used in the Stan models
trials <- do.call(rbind,by(trials,trials$sub_index,add_trials_nl))
trials$overall_trial_nl <- 1:nrow(trials) #get the overall trial number ignoring late trials and collapsing across subjects

#get mean-centered trial and block predictors for easier fitting in Stan
trials$trial_nl_cent <- trials$trial_nl - mean(trials$trial_nl)                                                          
trials$block_cent <- trials$block - mean(trials$block) 

#Create indices from 1:n_f for each fractal image. To do this, first create a mini-df with one column having all the fA_img values and the other two
#columns having indices for fA and fB. This assumes that every fA_img is paired with a unique fB_img.
f_index_df <- data.frame(fA_img = unique(trials$fA_img),fA_ix = 1:length(unique(trials$fA_img)),fB_ix = (1:length(unique(trials$fA_img))+length(unique(trials$fA_img))))
trials <- left_join(trials,f_index_df,by="fA_img")

#get the chosen fractal index
trials <- trials %>% mutate(chosen_frac = ifelse(choice == "fA",fA_ix,fB_ix))
trials <- trials %>% mutate(unchosen_frac = ifelse(choice == "fA",fB_ix,fA_ix))

#add a column with the affect probe number for each subject (999 if no probe response). These will be passed into Stan
trials <- add_probe_number(trials,newcol="dec_probe_number",val_col="dec_rate") #for decision probes
trials <- add_probe_number(trials,newcol="feed_probe_number",val_col="feed_rate") #for decision probes
```

# Data review

```{r}
percentage_plots(trials,"stay")
```

```{r}
choice_fit <- lmer(choice_numeric ~ fB_win_prob + fA_win_prob + (1|id),trials)
summary(choice_fit)
```

```{r}
stay_fit <- lmer(stay ~ chosen_out + unchosen_out + (1|id),trials)
summary(stay_fit)
```

```{r}
ages <- subs$age[subs$age != 999]
mean(ages)
sd(ages)
```

```{r}
sum(subs$gender=="Male")/sum(subs$gender!=999)
```

```{r}
sum(subs$gender=="Female")/sum(subs$gender!=999)
```

```{r}
sum(subs$gender=="Non-binary")/sum(subs$gender!=999)
```

# Choice models

Fitting Q-learning models to choice.

## Raw Q-learning

Fitting a basic Q-learning model with no nuisance parameters.
```{r 5}
basic_qlearn <- fit_stan_model(stan_file = paste0(stan_model_dir_s22,"basic_qlearn.stan"),
                         model_out_dir = model_out_dir,
                         raw_data = trials,
                         study = "s22fu",
                         skip=c("waic","check_csvs"),
                         n_t=104)
```

## Q-learning with autocorrelation

Now adding an autocorrelation component to the Q-learning model.
```{r}
autocor_qlearn <- fit_stan_model(stan_file = paste0(stan_model_dir_s22,"autocor_qlearn.stan"),
                         model_out_dir = model_out_dir,
                         raw_data = trials,
                         study = "s22fu",
                         skip=c("waic","check_csvs"),
                         n_t=104)
```

```{r}
fsml_compare(basic_qlearn,autocor_qlearn)
```
Autocorrelation model fits better.

## Q-learning with autocorrelation and side bias

Adding left-side bias as a parameter that influence choices.
```{r}
autocor_qlearn_sidebias <- fit_stan_model(stan_file = paste0(stan_model_dir_s22fu,"autocor_qlearn_sidebias.stan"),
                         model_out_dir = model_out_dir,
                         raw_data = trials,
                         study = "s22fu",
                         skip=c("waic","check_csvs"),
                         n_t=104)
```

```{r}
fsml_compare(autocor_qlearn,autocor_qlearn_sidebias)
```
Not sufficient evidence that the sidebias model fits better.

## Q-learning with autocorrelation and forgetting, no independent decay parameter

Update Q values of fractals not presented toward 0
```{r}
autocor_alphforget_qlearn <- fit_stan_model(stan_file = paste0(stan_model_dir_s22,"autocor_alphforget_qlearn.stan"),
                         model_out_dir = model_out_dir,
                         raw_data = trials,
                         study = "s22fu",
                         skip=c("waic","check_csvs"),
                         n_t=104)
```

```{r}
fsml_compare(autocor_qlearn,autocor_alphforget_qlearn)
```
Not sufficient evidence that this fits better.

## Q-learning with autocorrelation and forgetting, independent decay parameter

Update Q values of fractals not presented toward 0, using an independent learning rate for this process
```{r}
autocor_hierforget_qlearn <- fit_stan_model(stan_file = paste0(stan_model_dir_s22,"autocor_hierforget_qlearn.stan"),
                         model_out_dir = model_out_dir,
                         raw_data = trials,
                         study = "s22fu",
                         skip=c("waic","check_csvs"),
                         n_t=104)
```

```{r}
fsml_compare(autocor_qlearn,autocor_alphforget_qlearn,autocor_hierforget_qlearn)
```
The model with an independent decay parameter fits best.

## Winning choice model review

Verifying that the effect of reward on choice is consistently positive by looking at the subject-level mean beta values.
```{r}
ncp_mean_hist(autocor_hierforget_qlearn$sum,"beta")
```
Yes, it is.

Looking at the mean effects of each parameter.
```{r}
filt_sum(autocor_hierforget_qlearn$sum,"mu")
```

# Affect models

## Model comparison

First, fitting models that include variables only for one theory. These fill be fit without shrinkage priors, since they include only 2-3 predictors of interest each.

Simple value model
```{r}
simp_val_noshrink <- fit_stan_model(stan_file = paste0(stan_model_dir_s22fu,"simp_val_noshrink.stan"),
                                    model_out_dir = model_out_dir,
                                    raw_data = trials,
                                    study = "s22fu",
                                    skip=c("waic","check_csvs"),
                                    n_t=104)
```

PE model
```{r}
pe_noshrink <- fit_stan_model(stan_file = paste0(stan_model_dir_s22fu,"pe_noshrink.stan"),
                                    model_out_dir = model_out_dir,
                                    raw_data = trials,
                                    study = "s22fu",
                                    n_t=104)
```

Regret model
```{r}
regret_noshrink <- fit_stan_model(stan_file = paste0(stan_model_dir_s22fu,"regret_noshrink.stan"),
                                    model_out_dir = model_out_dir,
                                    raw_data = trials,
                                    study = "s22fu",
                                    skip=c("waic","check_csvs"),
                                    n_t=104)
```


Fitting the combined model (without a shrinkage prior, for greater comparability).
```{r}
combined_noshrink <- fit_stan_model(stan_file = paste0(stan_model_dir_s22fu,"combined_noshrink.stan"),
                                    model_out_dir = model_out_dir,
                                    raw_data = trials,
                                    study = "s22fu",
                                    skip=c("waic","check_csvs"),
                                    n_t=104)
```

Finally, fitting a null model (just nuisance parameters, no predictors of interest).
```{r}
null_affect <- fit_stan_model(stan_file = paste0(stan_model_dir_s22fu,"null_affect.stan"),
                         model_out_dir = model_out_dir,
                         raw_data = trials,
                         study = "s22fu",
                         skip=c("waic","check_csvs"),
                         n_t=104)
```

Now, comparing each theory-specific model to the null
```{r}
fsml_compare(simp_val_noshrink,null_affect)
fsml_compare(regret_noshrink,null_affect)
fsml_compare(pe_noshrink,null_affect)
```


Comparing each theory-specific model to the combined model.
```{r}
fsml_compare(combined_noshrink,pe_noshrink,regret_noshrink,simp_val_noshrink,null_affect)
```

## Posterior effect estimates

### Main analysis

Looking at the effect estimates for each of the individual variables.

First, refitting the combined model with a shrinkage prior.
```{r}
combined_shrink <- fit_stan_model(stan_file = paste0(stan_model_dir_s22fu,"combined_shrink.stan"),
                         model_out_dir = model_out_dir,
                         raw_data = trials,
                         study = "s22fu",
                         skip=c("waic","check_csvs"),
                         n_t=104)
```

Calculating R-squared for this model
```{r}
dec_rsq <- mcmc_rsq(combined_shrink$fit,"dec_pred","d_resid",print_plot=FALSE)
dec_rsq$sum
```

```{r}
feed_rsq <- mcmc_rsq(combined_shrink$fit,"feed_pred","f_resid",print_plot=FALSE)
feed_rsq$sum
```

Because two of the prediction error effects are in the wrong direction, it raises the question of whether the combined model still fits better without these prediction-inconsistent effects.
Refitting the model with no effects of TDE or RPE.
```{r}
combined_noshrink_noVQf <- fit_stan_model(stan_file = paste0(stan_model_dir_s22fu,"combined_noshrink_noVQf.stan"),
                         model_out_dir = model_out_dir,
                         raw_data = trials,
                         study = "s22fu",
                         skip=c("waic","check_csvs"),
                         n_t=104)
```

```{r}
fsml_compare(combined_noshrink_noVQf,pe_noshrink,regret_noshrink,simp_val_noshrink)
```
Yes, the combined model is still clearly superior.

### Exponential decay
How does the fit look when we assume exponentially decaying effects, as in Study 1?
```{r}
combined_shrink_exdec <- fit_stan_model(stan_file = paste0(stan_model_dir_s22fu,"combined_shrink_exdec.stan"),
                         model_out_dir = model_out_dir,
                         raw_data = trials,
                         study = "s22fu",
                         skip=c("waic","check_csvs"),
                         n_t=104)
```

```{r}
fsml_compare(combined_shrink,combined_shrink_exdec)
```

Not doing exponential decay works best

### Supplementary effect estimates

Estimating these effects a few different ways as a robustness check on the above estimates, and for desriptive purposes.

Looking at the effect estimates for the raw valence predictors
```{r}
mcmc_areas(
  cs_mudraws,
  area_method = "scaled height",
  prob = 0.5, # 80% intervals
  prob_outer = 0.9, # 99%
  point_est = "mean"
)
```

Running the combined model with no nuisance parameters added to the choice model
```{r}
combined_shrink_rawQ <- fit_stan_model(stan_file = paste0(stan_model_dir_s22fu,"combined_shrink_rawQ.stan"),
                         model_out_dir = model_out_dir,
                         raw_data = trials,
                         study = "s22fu",
                         skip=c("waic","check_csvs"),
                         n_t=104)
```

Plotting effects
```{r}
csr_mudraws <- get_draws_df(combined_shrink_rawQ$mod,combined_shrink_rawQ$fit,vars=c("dpiw_mu[1]","dpiw_mu[2]","dpiw_mu[3]","fpiw_mu[1]","fpiw_mu[2]","fpiw_mu[3]","fpiw_mu[4]"))

csr_mudraws_origform <- csr_mudraws %>% transmute(choice_TDE = -`dpiw_mu[1]`, Q_ch = `dpiw_mu[1]` + `dpiw_mu[2]` + `dpiw_mu[3]`, Q_regret=`dpiw_mu[3]`, 
                                                      PWRD = -`fpiw_mu[1]`,RPE = -`fpiw_mu[2]`,reward = `fpiw_mu[1]` + `fpiw_mu[2]` + `fpiw_mu[3]` + `fpiw_mu[4]`, regret = `fpiw_mu[4]`)
mcmc_areas(
  csr_mudraws_origform,
  area_method = "scaled height",
  prob = 0.5, # 80% intervals
  prob_outer = 0.9, # 99%
  point_est = "mean"
)
```

Now, examining effects from each theory-specific model.

Simple value model
```{r}
csr_mudraws <- get_draws_df(combined_shrink_rawQ$mod,combined_shrink_rawQ$fit,vars=c("dpiw_mu[1]","dpiw_mu[2]","dpiw_mu[3]","fpiw_mu[1]","fpiw_mu[2]","fpiw_mu[3]","fpiw_mu[4]"))

csr_mudraws_origform <- csr_mudraws %>% transmute(choice_TDE = -`dpiw_mu[1]`, Q_ch = `dpiw_mu[1]` + `dpiw_mu[2]` + `dpiw_mu[3]`, Q_regret=`dpiw_mu[3]`, 
                                                      PWRD = -`fpiw_mu[1]`,RPE = -`fpiw_mu[2]`,reward = `fpiw_mu[1]` + `fpiw_mu[2]` + `fpiw_mu[3]` + `fpiw_mu[4]`, regret = `fpiw_mu[4]`)
sv_draws <- simp_val_noshrink$fit$draws(c("dpiw_mu","fpiw_mu"))
mcmc_areas(
  sv_draws,
  area_method = "scaled height",
  prob = 0.5, # 80% intervals
  prob_outer = 0.9, # 99%
  point_est = "mean"
)
```

PE model
```{r}
pe_draws <- pe_noshrink$fit$draws(c("dpiw_mu","fpiw_mu"))
temp <- mcmc_areas(
  pe_draws,
  area_method = "scaled height",
  prob = 0.5, # 80% intervals
  prob_outer = 0.9, # 99%
  point_est = "mean"
  )
```

Regret model
```{r}
regret_draws <- regret_noshrink$fit$draws(c("dpiw_mu","fpiw_mu"))
mcmc_areas(
  regret_draws,
  area_method = "scaled height",
  prob = 0.5, # 80% intervals
  prob_outer = 0.9, # 99%
  point_est = "mean"
)
```
# Linear algebra reanalysis

```{r}
#Create direction vectors, where the dimensions are int he order:
#       Vb,Qcd,Qud,Vt,Qcf,rc,ru
tde <- c(-.5,.5,0,0,0,0,0)
q_ch <- c(0,1,0,0,0,0,0)
q_reg <- c(0,.5,-.5,0,0,0,0)
pwrd <- c(0,0,0,-.5,0,.5,0)
rpe <- c(0,0,0,0,-.5,.5,0)
rew <- c(0,0,0,0,0,1,0)
out_reg <- c(0,0,0,0,0,.5,-.5)
dvec_mat <- cbind(tde,q_ch,q_reg,pwrd,rpe,rew,out_reg) #turn into matrix

combined_shrink_fit <- load_cmdstan_fit(model_out_dir=model_out_dir,"combined_shrink")
cs_draws <- combined_shrink_fit$draws(c("dpiw_mu","fpiw_mu")) #get draws array
#vec_ws <- apply(cs_draws,c(1,2),vec_optim,dvec=dvec_mat) #get vector weights
#vec_ws_org <- aperm(vec_ws,c(2,3,1)) #rearrange to be in the same order that they were in the draws array
#dimnames(vec_ws_org)[[3]] <- c("tde","q_ch","q_reg","pwrd","rpe","rew","out_reg") #name meaningfully
#save(vec_ws_org,file=paste0(model_out_dir,"vec_ws_s2.RData"))
load(paste0(model_out_dir,"vec_ws_s2.RData"))
```

```{r}
#plot raw weights, in the order Q_ch,choice; V_block; Q_unch; r_ch;, Q_ch,out; V_trial; r_unch
raw_eff_intervals<- create_interval_plot(arr = cs_draws,names = c("fpiw_mu[4]","fpiw_mu[3]","fpiw_mu[2]","fpiw_mu[1]","dpiw_mu[3]","dpiw_mu[2]","dpiw_mu[1]"),
                                         color_mapping = c("fpiw_mu[1]" = "#414141","fpiw_mu[2]" = "#414141","fpiw_mu[3]" = "#414141","fpiw_mu[4]" = "#414141",
                                                           "dpiw_mu[1]" = "#414141","dpiw_mu[2]" = "#414141","dpiw_mu[3]" = "#414141",
                                                           "fpiw_mu[1]_med" = "black","fpiw_mu[2]_med" = "black","fpiw_mu[3]_med" = "black","fpiw_mu[4]_med" = "black",
                                                           "dpiw_mu[1]_med" = "black","dpiw_mu[2]_med" = "black","dpiw_mu[3]_med" = "black"),
                                          xmin = -.2,xmax = .85,
                                          percentiles = c(0.025,.25,.50,.75,.975),
                                          dot_size = 3) +
                   theme(panel.grid.major.x = element_line(color = "#DCDCDC", size = .47),
                         panel.background = element_rect(fill = "white", color = NA))
raw_eff_intervals_s2 <- raw_eff_intervals
save(raw_eff_intervals_s2,file="~/Documents/active_manuscripts/s22/figures/raw_eff_intervals_s2.RData")
```
```{r}
#recode this draws array to match the preregistered formulation of the model
cs_draws_prereg <- array(NA, dim = c(1000, 4, 7))
dimnames(cs_draws_prereg) <- list(NULL, NULL, c("SV_ch", "PE_ch", "CC_ch", "SV_out", "PE_outV", "PE_outQ", "CC_out"))
cs_draws_prereg[,,"SV_ch"] <- cs_draws[,,"dpiw_mu[1]"] + cs_draws[,,"dpiw_mu[2]"] + cs_draws[,,"dpiw_mu[3]"]
cs_draws_prereg[,,"PE_ch"] <- -cs_draws[,,"dpiw_mu[1]"]
cs_draws_prereg[,,"CC_ch"] <- -cs_draws[,,"dpiw_mu[3]"]
cs_draws_prereg[,,"SV_out"] <- cs_draws[,,"fpiw_mu[1]"] + cs_draws[,,"fpiw_mu[2]"] + cs_draws[,,"fpiw_mu[3]"] + cs_draws[,,"fpiw_mu[4]"]
cs_draws_prereg[,,"PE_outV"] <- -cs_draws[,,"fpiw_mu[1]"]
cs_draws_prereg[,,"PE_outQ"] <- -cs_draws[,,"fpiw_mu[2]"]
cs_draws_prereg[,,"CC_out"] <- -cs_draws[,,"fpiw_mu[4]"]

raw_eff_intervals_prereg <- create_interval_plot(arr = cs_draws_prereg,names = c("CC_out","PE_outQ","PE_outV","SV_out","CC_ch","PE_ch","SV_ch"),
                                          color_mapping = c("SV_ch" = "#5C9ED6","PE_ch" = "#8B5FBF","CC_ch"="#D9534F",
                                           "SV_ch_med" = "#39658D", "PE_ch_med" = "#4C2F79", "CC_ch_med" = "#8C2829",
                                           "SV_out" = "#5C9ED6","PE_outQ" = "#8B5FBF","PE_outV" = "#8B5FBF","CC_out"="#D9534F",
                                           "SV_out_med" = "#39658D", "PE_outQ_med" = "#4C2F79","PE_outV_med" = "#4C2F79", "CC_out_med" = "#8C2829"),
                                          xmin = -.2,xmax = .9,
                                          percentiles = c(0.025,.25,.50,.75,.975),
                                          dot_size = 3) +
                   theme(panel.grid.major.x = element_line(color = "#DCDCDC", size = .47),
                         panel.background = element_rect(fill = "white", color = NA),
                         axis.text.x = element_blank(),
                         axis.text.y=element_blank())
ggsave("~/Documents/active_manuscripts/s22/figures/raw_eff_intervals_prereg.pdf",raw_eff_intervals_prereg,width=1.8,height=1.35)
```

```{r}
quantile(cs_draws_prereg[,,"SV_ch"],c(.025,.50,.975))
mean(cs_draws_prereg[,,"SV_ch"] > 0)

quantile(cs_draws_prereg[,,"SV_out"],c(.025,.50,.975))
mean(cs_draws_prereg[,,"SV_out"] > 0)

quantile(cs_draws_prereg[,,"PE_outV"],c(.025,.50,.975))
mean(cs_draws_prereg[,,"PE_outV"] > 0)

quantile(cs_draws_prereg[,,"CC_out"],c(.025,.50,.975))
mean(cs_draws_prereg[,,"CC_out"] > 0)
```

                                                          
```{r}
#plot normed weights
mn_data <- apply(cs_draws,c(1,2),function(x) sum(abs(x))) #get the manhattan norms for each data vector by summing the absolute values

comb_array <- abind(vec_ws_org,mn_data,along=3) #staple mn_data to the back of the third dimension of the vector weight array
vec_ws_norm <- apply(comb_array,c(1,2), get_ports) #get portions of relationship accounted for

#have to rearrange and name again
vec_ws_norm_org <- aperm(vec_ws_norm,c(2,3,1)) 
dimnames(vec_ws_norm_org)[[3]] <- c("tde","q_ch","q_reg","pwrd","rpe","rew","out_reg","resid") #name meaningfully
```

Get stats
```{r}
quantile(vec_ws_norm_org[,,"q_ch"],c(.50,.025,.975))
round(mean(vec_ws_norm_org[,,"q_ch"] > 0) * 100,1)

quantile(vec_ws_norm_org[,,"rew"],c(.50,.025,.975))
round(mean(vec_ws_norm_org[,,"rew"] > 0) * 100,1)

quantile(vec_ws_norm_org[,,"pwrd"],c(.50,.025,.975))
round(mean(vec_ws_norm_org[,,"pwrd"] > 0) * 100,1)

quantile(vec_ws_norm_org[,,"out_reg"],c(.50,.025,.975))
round(mean(vec_ws_norm_org[,,"out_reg"] > 0) * 100,1)
```

Plot intervals
```{r}
#create interval plots for manhattan norm ratios
variable_intervals_s2<- create_interval_plot(arr = vec_ws_norm_org,names = c("out_reg","pwrd","rpe","rew","q_reg","tde","q_ch"),
                                          xmin = -.012,xmax = .48) +
                        theme(panel.grid.major.x = element_line(color = "#DCDCDC", size = .47),
                              panel.background = element_rect(fill = "white", color = NA))
save(variable_intervals_s2,file="~/Documents/active_manuscripts/s22/figures/variable_intervals_s2.RData")
```


```{r}
#get sub-arrays of normed weights for each theory
sv_ws <- vec_ws_norm_org[,,c(2,6)] 
pe_ws <- vec_ws_norm_org[,,c(1,4,5)] 
reg_ws <- vec_ws_norm_org[,,c(3,7)] 

#sum them to get total theory weights
sv_ports <- apply(sv_ws,c(1,2),sum) 
pe_ports <- apply(pe_ws,c(1,2),sum)
reg_ports <- apply(reg_ws,c(1,2),sum)

thr_ports <- array(c(sv_ports,pe_ports,reg_ports,vec_ws_norm_org[,,8]),dim=c(1000,4,4))
dimnames(thr_ports)[[3]] <- c("sv","pe","cc","resid") #name meaningfully
```

Get stats for theory vectors
```{r}
quantile(thr_ports[,,"sv"],c(.50,.025,.975))
round(mean(thr_ports[,,"sv"] > 0) * 100,1)

quantile(thr_ports[,,"pe"],c(.50,.025,.975))
round(mean(thr_ports[,,"pe"] > 0) * 100,1)

quantile(thr_ports[,,"cc"],c(.50,.025,.975))
round(mean(thr_ports[,,"cc"] > 0) * 100,1)
```
Calculate differences between theory vectors, getting stats
```{r}
sv_cc <- thr_ports[,,"sv"] - thr_ports[,,"cc"]
sv_pe <- thr_ports[,,"sv"] - thr_ports[,,"pe"]
cc_pe <- thr_ports[,,"cc"] - thr_ports[,,"pe"]

quantile(cc_pe,c(.025,.975,.50))
round(sum(cc_pe > 0)/4000 * 100,1)

quantile(sv_cc,c(.025,.975,.50))
round(sum(sv_cc > 0)/4000 * 100,1)

quantile(sv_pe,c(.025,.975,.50))
round(sum(sv_pe > 0)/4000 * 100,1)
```

Plot theory vectors
```{r}
theory_intervals_s2 <- create_interval_plot(arr = thr_ports,names = c("resid","cc","pe","sv"),
                                          xmin = -.012,xmax = .8) +
                        theme(panel.grid.major.x = element_line(color = "#DCDCDC", size = 1),
                              panel.background = element_rect(fill = "white", color = NA))
save(theory_intervals_s2,file="~/Documents/active_manuscripts/s22/figures/theory_intervals_s2.RData")
```

```{r}
#Get the likelihood of each theory combination...
draw_codes <- apply(thr_ports[,,-4],c(1,2),code_draw) #code each draw according to which theories are non-zero
#get percentage likelihoods for each theory combination
paste0("Simple value, PE, regret: ",100*sum(draw_codes == "123")/length(draw_codes))
paste0("Simple value, PE: ",100*sum(draw_codes == "12")/length(draw_codes))
paste0("Simple value, regret: ",100*sum(draw_codes == "13")/length(draw_codes))
paste0("PE, regret: ",100*sum(draw_codes == "23")/length(draw_codes))
paste0("Simple value: ",100*sum(draw_codes == "1")/length(draw_codes))
paste0("PE: ",100*sum(draw_codes == "2")/length(draw_codes))
paste0("Regret: ",100*sum(draw_codes == "3")/length(draw_codes))
```

## Compare vector lengths across studies

Variable vectors
```{r}
# load("~/projects/spring_2022_study/output/results/stan_model_fits/final_models/vec_ws_norm_org_s1.RData") #load in Study 1 vectors
# diff_var_vecs <- vec_ws_norm_org - vec_ws_norm_org_s1
# save(diff_var_vecs,file=paste0(model_out_dir,"diff_var_vecs.RData"))
load(paste0(model_out_dir,"diff_var_vecs.RData"))


quantile(diff_var_vecs[,,"rew"],c(.025,.975,.50))
round(sum(diff_var_vecs[,,"rew"] > 0)/4000 * 100,1)

quantile(diff_var_vecs[,,"q_ch"],c(.025,.975,.50))
round(sum(diff_var_vecs[,,"q_ch"] > 0)/4000 * 100,1)

quantile(diff_var_vecs[,,"rpe"],c(.025,.975,.50))
round(sum(diff_var_vecs[,,"rpe"] < 0)/4000 * 100,1)

quantile(diff_var_vecs[,,"out_reg"],c(.025,.975,.50))
round(sum(diff_var_vecs[,,"out_reg"] < 0)/4000 * 100,1)

quantile(diff_var_vecs[,,"pwrd"],c(.025,.975,.50))
round(sum(diff_var_vecs[,,"pwrd"] < 0)/4000 * 100,1)

quantile(diff_var_vecs[,,"q_reg"],c(.025,.975,.50))
round(sum(diff_var_vecs[,,"q_reg"] < 0)/4000 * 100,1)

quantile(diff_var_vecs[,,"tde"],c(.025,.975,.50))
round(sum(diff_var_vecs[,,"tde"] < 0)/4000 * 100,1)

```

Theory vectors
```{r}
# load("~/projects/spring_2022_study/output/results/stan_model_fits/final_models/thr_ports_s1.RData")
# diff_theory_vecs <- thr_ports - thr_ports_s1
# save(diff_theory_vecs,file=paste0(model_out_dir,"diff_theory_vecs.RData"))
load(paste0(model_out_dir,"diff_theory_vecs.RData"))
quantile(diff_theory_vecs[,,"Simple value"],c(.025,.975,.50))
round(sum(diff_theory_vecs[,,"Simple value"] > 0)/4000 * 100,1)

quantile(diff_theory_vecs[,,"Regret"],c(.025,.975,.50))
round(sum(diff_theory_vecs[,,"Regret"] < 0)/4000 * 100,1)

quantile(diff_theory_vecs[,,"PE"],c(.025,.975,.50))
round(sum(diff_theory_vecs[,,"PE"] < 0)/4000 * 100,1)
```

# Plot fit vectors
Create a plot of the expected effect vector, the best approximation to this vector, and uncertainties in variable vector lengths and the effect vector.
```{r}
eff_mean <- apply(cs_draws,3,mean) #get mean effect vector

#approximate this vector
mean_vec_ws <- vec_optim(eff_mean,dvec_mat)
mean_vecs <- matrix(NA,nrow=nrow(dvec_mat),ncol=ncol(dvec_mat))
for(i in 1:ncol(mean_vecs)){
  mean_vecs[,i] <- mean_vec_ws[i]*dvec_mat[,i]
}
resid_mean <- eff_mean - rowSums(mean_vecs)


#make the order PE, CC, SV, with RPE before PWRD
mean_vecs <- mean_vecs[,c(1,3,2,5,4,7,6)]

vec_theory <- c("PE","CC","SV","PE","PE","CC","SV") #theories to which the vectors belong
                              
vec_sol_ch1 <- vec_sol_plot(dimx=1,dimy=2,vec_theory,mean_vecs,eff_mean,resid_mean,eff_draws=cs_draws,ci=0.95)
vec_sol_ch2 <- vec_sol_plot(dimx=3,dimy=2,vec_theory,mean_vecs,eff_mean,resid_mean,eff_draws=cs_draws,ci=0.95,add_resid=F)
vec_sol_out1 <- vec_sol_plot(dimx=5,dimy=6,vec_theory,mean_vecs,eff_mean,resid_mean,eff_draws=cs_draws,ci=0.95)
vec_sol_out2 <- vec_sol_plot(dimx=4,dimy=7,vec_theory,mean_vecs,eff_mean,resid_mean,eff_draws=cs_draws,ci=0.95)
             

ggsave("~/Documents/active_manuscripts/s22/figures/vec_sol_ch1_fu.pdf",vec_sol_ch1,width=2,height=2)
ggsave("~/Documents/active_manuscripts/s22/figures/vec_sol_ch2_fu.pdf",vec_sol_ch2,width=2,height=2)
ggsave("~/Documents/active_manuscripts/s22/figures/vec_sol_out1_fu.pdf",vec_sol_out1,width=2,height=2)
ggsave("~/Documents/active_manuscripts/s22/figures/vec_sol_out2_fu.pdf",vec_sol_out2,width=2,height=2)
```

# Do linear algebra analysis on model with raw Q values
```{r}
#Create direction vectors, where the dimensions are int he order:
#       Vb,Qcd,Qud,Vt,Qcf,rc,ru
tde <- c(-.5,.5,0,0,0,0,0)
q_ch <- c(0,1,0,0,0,0,0)
q_reg <- c(0,.5,-.5,0,0,0,0)
pwrd <- c(0,0,0,-.5,0,.5,0)
rpe <- c(0,0,0,0,-.5,.5,0)
rew <- c(0,0,0,0,0,1,0)
out_reg <- c(0,0,0,0,0,.5,-.5)
dvec_mat <- cbind(tde,q_ch,q_reg,pwrd,rpe,rew,out_reg) #turn into matrix

combined_shrink_rawQ_fit <- load_cmdstan_fit(model_out_dir=model_out_dir,"combined_shrink_rawQ")
csr_draws <- combined_shrink_rawQ_fit$draws(c("dpiw_mu","fpiw_mu")) #get draws array

vec_ws_rq <- apply(csr_draws,c(1,2),vec_optim,dvec=dvec_mat) #get vector weights
vec_ws_org_rq <- aperm(vec_ws_rq,c(2,3,1)) #rearrange to be in the same order that they were in the draws array
dimnames(vec_ws_org_rq)[[3]] <- c("tde","q_ch","q_reg","pwrd","rpe","rew","out_reg") #name meaningfully
#save(vec_ws_org_rq,file=paste0(model_out_dir,"vec_ws_rq_s2.RData"))
load(paste0(model_out_dir,"vec_ws_s2.RData"))
```


```{r}
#plot normed weights
mn_data_rq<- apply(csr_draws,c(1,2),function(x) sum(abs(x))) #get the manhattan norms for each data vector by summing the absolute values

comb_array_rq <- abind(vec_ws_org_rq,mn_data_rq,along=3) #staple mn_data to the back of the third dimension of the vector weight array
vec_ws_norm_rq <- apply(comb_array_rq,c(1,2), get_ports) #get portions of relationship accounted for

#have to rearrange and name again
vec_ws_norm_org_rq <- aperm(vec_ws_norm_rq,c(2,3,1)) 
dimnames(vec_ws_norm_org_rq)[[3]] <- c("tde","q_ch","q_reg","pwrd","rpe","rew","out_reg","resid") #name meaningfully
```

Plot intervals
```{r}
#create interval plots for manhattan norm ratios
variable_intervals_s2<- create_interval_plot(arr = vec_ws_norm_org_rq,names = c("out_reg","pwrd","rpe","rew","q_reg","tde","q_ch"),
                                          xmin = -.012,xmax = .48)
save(variable_intervals_s2,file="~/Documents/active_manuscript/manuscript/figures/variable_intervals_s2.RData")
```


```{r}
#get sub-arrays of normed weights for each theory
sv_ws_rq <- vec_ws_norm_org_rq[,,c(2,6)] 
pe_ws_rq <- vec_ws_norm_org_rq[,,c(1,4,5)] 
reg_ws_rq <- vec_ws_norm_org_rq[,,c(3,7)] 

#sum them to get total theory weights
sv_ports_rq <- apply(sv_ws_rq,c(1,2),sum) 
pe_ports_rq <- apply(pe_ws_rq,c(1,2),sum)
reg_ports_rq <- apply(reg_ws_rq,c(1,2),sum)

thr_ports_rq <- array(c(sv_ports_rq,pe_ports_rq,reg_ports_rq,vec_ws_norm_org_rq[,,8]),dim=c(1000,4,4))
dimnames(thr_ports_rq)[[3]] <- c("sv","pe","cc","resid") #name meaningfully
```

Plot theory vectors
```{r}
theory_intervals_s2 <- create_interval_plot(arr = thr_ports_rq,names = c("resid","cc","pe","sv"),
                                          xmin = -.012,xmax = .8)
save(theory_intervals_s2,file="~/Documents/active_manuscript/manuscript/figures/theory_intervals_s2.RData")
```

```{r}
#Get the likelihood of each theory combination...

draw_codes <- apply(thr_ports[,,-4],c(1,2),code_draw) #code each draw according to which theories are non-zero
#get percentage likelihoods for each theory combination
paste0("Simple value, PE, regret: ",100*sum(draw_codes == "123")/length(draw_codes))
paste0("Simple value, PE: ",100*sum(draw_codes == "12")/length(draw_codes))
paste0("Simple value, regret: ",100*sum(draw_codes == "13")/length(draw_codes))
paste0("PE, regret: ",100*sum(draw_codes == "23")/length(draw_codes))
paste0("Simple value: ",100*sum(draw_codes == "1")/length(draw_codes))
paste0("PE: ",100*sum(draw_codes == "2")/length(draw_codes))
paste0("Regret: ",100*sum(draw_codes == "3")/length(draw_codes))
```

# ARL analyses

First, fit a model to raw valence ratings (at feedback). 
```{r}
trials_frate <- trials %>% filter(block_feedrate == 1) #get only trials from feedback rating blocks

ratings_arl <- fit_stan_model(stan_file = paste0(stan_model_dir_s22fu,"ratings_arl.stan"),
                         model_out_dir = model_out_dir,
                         raw_data = trials_frate,
                         study = "s22fu",
                         n_t=52,
                         iter_sampling = 2000)

ra_scd_draws <- get_draws("ratings_arl",model_out_dir=model_out_dir,vars=c("scd_beta_mu","scd_aff_sens_mu"))

quantile(ra_scd_draws[,,"scd_beta_mu"],c(.025,.50,.975))
mean(ra_scd_draws[,,"scd_beta_mu"] > 0)
quantile(ra_scd_draws[,,"scd_aff_sens_mu"],c(.025,.50,.975))
mean(ra_scd_draws[,,"scd_aff_sens_mu"] > 0)
```

Next, to model-predicted valence ratings (at feedback)
```{r}
model_pred_arl <- fit_stan_model(stan_file = paste0(stan_model_dir_s22fu,"model_pred_arl.stan"),
                         model_out_dir = model_out_dir,
                         raw_data = trials,
                         study = "s22fu",
                         n_t=104,
                         chains = 3)

mpa_asens_draws <- model_pred_arl$fit$draws("aff_sens_mu")
mcmc_areas(
  mpa_asens_draws,
  area_method = "scaled height",
  prob = 0.9,
  prob_outer = 0.99,
  point_est = "median"
) +
  coord_cartesian(xlim=c(0,NA))

mpa_beta_draws <- model_pred_arl$fit$draws("beta_mu")
mcmc_areas(
  mpa_beta_draws,
  area_method = "scaled height",
  prob = 0.9,
  prob_outer = 0.99,
  point_est = "median"
) +
  coord_cartesian(xlim=c(0,NA))
```

Finally, to model-predicted valence with nuisance parameters not part of the prediction
```{r}
model_pred_arl_nonuis <- fit_stan_model(stan_file = paste0(stan_model_dir_s22fu,"model_pred_arl_nonuis.stan"),
                         model_out_dir = model_out_dir,
                         raw_data = trials,
                         study = "s22fu",
                         iter_sampling = 1500,
                         n_t=104)
```

```{r}
mpan_scds <- get_draws("model_pred_arl_nonuis",model_out_dir=model_out_dir,vars=c("scd_beta_mu","scd_aff_sens_mu"))
quantile(mpan_scds[,,"scd_beta_mu"],c(.025,.5,.975))
mean(mpan_scds[,,"scd_beta_mu"] > 0)
quantile(mpan_scds[,,"scd_aff_sens_mu"],c(.025,.5,.975))
mean(mpan_scds[,,"scd_aff_sens_mu"] > 0)
quantile(mpan_scds[,,"scd_beta_mu"] - mpan_scds[,,"scd_aff_sens_mu"],c(.025,.5,.975))
mean(mpan_scds[,,"scd_beta_mu"] > mpan_scds[,,"scd_aff_sens_mu"])

```

This model makes the most sense - use this one as your main model.

## Variance clamping
You fit model versions that estimated the impact of A and R on choice by seeing how much variance in choice
probabilities was reduced when you assumed no effect of these variables. In the end, you decided that using
the scaled effects, as in the above, was a better choice. But leaving these here anyway...

```{r}
ratings_arl_vars <- fit_stan_model(stan_file = paste0(stan_model_dir_s22fu,"ratings_arl_vars.stan"),
                         model_out_dir = model_out_dir,
                         raw_data = trials_frate,
                         study = "s22fu",
                         n_t=52,
                         chains = 3)
```

```{r}
rav_draws <- get_draws("ratings_arl_vars",model_out_dir=model_out_dir,vars=c("cp_var_mu","cp_var_noA_mu","cp_var_noQ_mu","cp_var_noC_mu"))
rav_draws_diff<- (rav_draws[,,"cp_var_mu"]-rav_draws[,,"cp_var_noQ_mu"]) - (rav_draws[,,"cp_var_mu"]-rav_draws[,,"cp_var_noA_mu"])
quantile(rav_draws_diff,probs=c(.01,.025,.05,.5,.95,.975,.99))
```

```{r}
model_pred_arl_vars <- fit_stan_model(stan_file = paste0(stan_model_dir_s22fu,"model_pred_arl_vars.stan"),
                         model_out_dir = model_out_dir,
                         raw_data = trials,
                         study = "s22fu",
                         n_t=104,
                         chains = 3)
```

```{r}
mav_draws <- get_draws("model_pred_arl_vars",model_out_dir=model_out_dir,vars=c("cp_var_mu","cp_var_noA_mu","cp_var_noQ_mu","cp_var_noC_mu"))
mav_draws_diff<- (mav_draws[,,"cp_var_mu"]-mav_draws[,,"cp_var_noQ_mu"]) - (mav_draws[,,"cp_var_mu"]-mav_draws[,,"cp_var_noA_mu"])
quantile(mav_draws_diff,probs=c(.01,.025,.05,.5,.95,.975,.99))
```



# Exploratory analyses

This is sort of the "full" model, including effects of A and R in addition to reisdual affect and nuisance affect.
```{r}
breakdown_s2 <- fit_stan_model(stan_file = paste0(stan_model_dir_s22fu,"breakdown_s2.stan"),
                         model_out_dir = model_out_dir,
                         raw_data = trials_frate,
                         study = "s22fu",
                         n_t=52,
                         chains = 3)
```

```{r}
breakdown_s2$diagnostics
filt_sum(breakdown_s2$sum,"mu")
filt_sum(breakdown_s2$sum,"sigma")
```
Affect sensitivity parameters are out of left field. Big SDs on mean estimates - plus big sigmas - suggests that
we're just not powered to detect all of these effects. 


Regressing valence and reward onto choice, as a model-free confirmation that both influence choice.
```{r}
trials_frate_scale <- trials %>% filter(!is.na(feed_rate_z)) %>% mutate(feed_rate_z=scale(feed_rate_z),chosen_out=scale(chosen_out),unchosen_out=scale(unchosen_out))
stay_fit <- lmer(stay ~ feed_rate_z + chosen_out + unchosen_out + (feed_rate_z + chosen_out + unchosen_out | sub_index),trials_frate_scale)
summary(stay_fit)
confint(stay_fit, parm = c("feed_rate_z","chosen_out"))
```

# Effects of affect variables on RL
Get the effects of valence predictors on RL. You later decided these analyses didn't quite make sense, but I'll still keep them here.
```{r}
#get posteriors for affect regressors and affect sensitivity
aff_posts <- get_draws("model_pred_arl_nonuis",model_out_dir,vars=c("fpiw_mu[1]","fpiw_mu[2]","fpiw_mu[3]","fpiw_mu[4]"))
asens_post <- get_draws("model_pred_arl_nonuis",model_out_dir,vars=c("aff_sens_mu"))

#multiply each coefficient posterior by affect sensitity to get affect-mediated RL influence
aff_ch_list <- lapply(c(1:dim(aff_posts)[3]),function(dim) drop(aff_posts[,,dim])*drop(asens_post))
aff_ch <- abind(aff_ch_list,along=3)

#add the non-affect-mediated RL influence of reward to that posterior
beta_post <- get_draws("model_pred_arl_nonuis",model_out_dir,vars=c("beta_mu")) 
aff_ch[,,3] <- aff_ch[,,3] + drop(beta_post) #add to the reward posterior the effect of reward per se



pwrd <- c(-.5,0,.5,0)
rpe <- c(0,-.5,.5,0)
rew <- c(0,0,1,0)
out_reg <- c(0,0,.5,-.5)
dvec_val_mat <- cbind(pwrd,rpe,rew,out_reg) #turn into matrix

val_vec_ws <- apply(aff_ch,c(1,2),vec_optim,dvec=dvec_val_mat,init_pars=rep(0,4)) #get vector weights
vv_ws_org <- aperm(val_vec_ws,c(2,3,1)) #rearrange dims to be in the same order that they were in the draws array
dimnames(vv_ws_org)[[3]] <- c("pwrd","rpe","rew","out_reg") #name meaningfully
```

Get array of manhattan norm ratios
```{r}
#get array of manhattan norm ratios
mn_data_vv <- apply(aff_ch,c(1,2),function(x) sum(abs(x))) #get the manhattan norms for each data vector by summing the absolute values

comb_array_vv <- abind(vv_ws_org,mn_data_vv,along=3) #staple mn_data to the back of the third dimension of the vector weight array
vv_ws_norm <- apply(comb_array_vv,c(1,2), get_ports) #get portions of relationship accounted for

#have to rearrange and name again
vv_ws_norm_org <- aperm(vv_ws_norm,c(2,3,1)) 
dimnames(vv_ws_norm_org)[[3]] <- c("pwrd","rpe","rew","out_reg","resid") #name meaningfully

#create interval plots
create_interval_plot(arr = vv_ws_norm_org,names=c("pwrd","rpe","rew","out_reg","resid"),xmin = -.012,xmax = 1)
```

```{r}
#save(vv_ws_norm_org,file=paste0(model_out_dir,"vv_ws_norm_org_s2.RData"))
load(paste0(model_out_dir,"vv_ws_norm_org_s2.RData"))

#get total theory weights
sv_vv_ports <- vv_ws_norm_org[,,3] 
reg_vv_ports <- vv_ws_norm_org[,,4] 

pe_vv_ws <- vv_ws_norm_org[,,c(1,2)] 
pe_vv_ports <- apply(pe_vv_ws,c(1,2),sum)

thr_vv_ports <- array(c(sv_vv_ports,pe_vv_ports,reg_vv_ports,vv_ws_norm_org[,,5]),dim=c(1500,4,4))
dimnames(thr_vv_ports)[[3]] <- c("sv","pe","cc","resid") #name meaningfully

thr_vv_ports_s2 <- thr_vv_ports
save(thr_vv_ports_s2,file=paste0(model_out_dir,"thr_vv_ports_s2.RData"))
```

```{r}
theory_intervals_vv <- create_interval_plot(arr = thr_vv_ports,names = c("resid","cc","pe","sv"),
                                          xmin = -.017,xmax = 1)
theory_intervals_vv_s2 <- theory_intervals_vv
save(theory_intervals_vv_s2,file="~/Documents/active_manuscripts/s22/figures/theory_intervals_vv_s2.RData")
```

make the above vectors into theory vectors
```{r}
#s1
sv_ws <- vec_ws_norm_org[,,c(2,6)] 
pe_ws <- vec_ws_norm_org[,,c(1,4,5)] 
reg_ws <- vec_ws_norm_org[,,c(3,7)] 

#sum them to get total theory weights
sv_ports <- apply(sv_ws,c(1,2),sum) 
pe_ports <- apply(pe_ws,c(1,2),sum)
reg_ports <- apply(reg_ws,c(1,2),sum)

thr_ports <- array(c(sv_ports,pe_ports,reg_ports,vec_ws_norm_org[,,8]),dim=c(1000,4,4))
dimnames(thr_ports)[[3]] <- c("sv","pe","cc","resid") #name meaningfully
```

Get stats for theory vectors
```{r}
quantile(thr_ports[,,"sv"],c(.50,.025,.975))
round(mean(thr_ports[,,"sv"] > 0) * 100,1)

quantile(thr_ports[,,"pe"],c(.50,.025,.975))
round(mean(thr_ports[,,"pe"] > 0) * 100,1)

quantile(thr_ports[,,"cc"],c(.50,.025,.975))
round(mean(thr_ports[,,"cc"] > 0) * 100,1)
```


```{r}
rl_var_free_s2 <- fit_stan_model(stan_file = paste0(stan_model_dir_s22fu,"rl_var_free_s2.stan"),
                         model_out_dir = model_out_dir,
                         raw_data = trials,
                         study = "s22fu",
                         n_t=104)
```

#Theory predictions only ARL
Rerun the winning ARL model (model_pred_arl_nonuis) using only the portion of valence explained by the 
SV, CC, and PE theories to update affect associations A.
```{r}
arl_theor_pred_s2 <- fit_stan_model(stan_file = paste0(stan_model_dir_s22fu,"arl_theor_pred_s2.stan"),
                         model_out_dir = model_out_dir,
                         raw_data = trials,
                         study = "s22fu",
                         adapt_delta = .95,
                         max_treedepth = 12,
                         iter_sampling = 1500,
                         n_t=104)
                                         
```

```{r}
atps2_scds <- get_draws("arl_theor_pred_s2",model_out_dir=model_out_dir,vars=c("scd_beta_mu","scd_aff_sens_mu"))
quantile(atps2_scds[,,"scd_beta_mu"],c(.025,.5,.975))
mean(atps2_scds[,,"scd_beta_mu"] > 0)
quantile(atps2_scds[,,"scd_aff_sens_mu"],c(.025,.5,.975))
mean(atps2_scds[,,"scd_aff_sens_mu"] > 0)
```

#Utility vs. reinforcement test
Testing the hypothesis that valence reflects utility rather than reinforcement.

A version of the ARL model that assumes affect is reinforcement
```{r}
model_pred_arl_nonuis_pe <- fit_stan_model(stan_file = paste0(stan_model_dir_s22fu,"model_pred_arl_nonuis_pe.stan"),
                         model_out_dir = model_out_dir,
                         raw_data = trials,
                         study = "s22fu",
                         iter_sampling = 1500,
                         n_t=104)
model_pred_arl_nonuis_pe <- read_fsml("model_pred_arl_nonuis_pe",model_out_dir=model_out_dir)
```

```{r}
fsml_compare(model_pred_arl_nonuis_pe,model_pred_arl_nonuis)
```
Something went very wrong with the model_pred_arl_nonuis_pe fit. Checking if it was a mistake in the code, or if it's the update itself, by rerunning it with the update switched back to a utility-based formulation.

```{r}
mpan_sw_pe <- fit_stan_model(stan_file = paste0(stan_model_dir_s22fu,"mpan_sw_pe.stan"),
                         model_out_dir = model_out_dir,
                         raw_data = trials,
                         study = "s22fu",
                         iter_sampling = 1500,
                         n_t=104)
```

This runs fine, so it was the fact that PEs were used for the update. It looks like the third chain was particularly
problematic. So let's rerun loo with only chains 1, 2, and 4, and see if the k values are better and the comparison
is more reasonable.


```{r}
mpan_124_loo <- get_loo_from_csvs(files=c("/Users/dp/projects/s22_follow_up/output/results/stan_model_fits/model_pred_arl_nonuis/model_pred_arl_nonuis-202312281608-1-1c8417.csv","/Users/dp/projects/s22_follow_up/output/results/stan_model_fits/model_pred_arl_nonuis/model_pred_arl_nonuis-202312281608-2-1c8417.csv","/Users/dp/projects/s22_follow_up/output/results/stan_model_fits/model_pred_arl_nonuis/model_pred_arl_nonuis-202312281608-4-1c8417.csv"),variables=c("choice_lik","affect_lik"))
mpan_pe_124_loo <- get_loo_from_csvs(files=c("/Users/dp/projects/s22_follow_up/output/results/stan_model_fits/model_pred_arl_nonuis_pe/model_pred_arl_nonuis_pe-202405201910-1-0c3a4d.csv","/Users/dp/projects/s22_follow_up/output/results/stan_model_fits/model_pred_arl_nonuis_pe/model_pred_arl_nonuis_pe-202405201910-2-0c3a4d.csv","/Users/dp/projects/s22_follow_up/output/results/stan_model_fits/model_pred_arl_nonuis_pe/model_pred_arl_nonuis_pe-202405201910-4-0c3a4d.csv"),variables=c("choice_lik","affect_lik"))

mpan_pe_124_loo
loo_compare(mpan_124_loo,mpan_pe_124_loo)
```
Yep, things are normal again when the third chain is deleted. And the utility model fits better by 3.4 SEs.

Now, do a model comparison using raw ratings rather than model-predicted affect
```{r}
trials_frate <- trials %>% filter(block_feedrate == 1) #get only trials from feedback rating blocks

ratings_arl_pe <- fit_stan_model(stan_file = paste0(stan_model_dir_s22fu,"ratings_arl_pe.stan"),
                         model_out_dir = model_out_dir,
                         raw_data = trials_frate,
                         study = "s22fu",
                         n_t=52,
                         iter_sampling = 2000)
```

```{r}
fsml_compare(ratings_arl_pe,ratings_arl)
```

Redo the comparison with no Q values
```{r}
ratings_arl_noQ_pe <- fit_stan_model(stan_file = paste0(stan_model_dir_s22fu,"ratings_arl_noQ_pe.stan"),
                         model_out_dir = model_out_dir,
                         raw_data = trials_frate,
                         study = "s22fu",
                         n_t=52,
                         chains = 3)
```

```{r}
ratings_arl_noQ <- fit_stan_model(stan_file = paste0(stan_model_dir_s22fu,"ratings_arl_noQ.stan"),
                         model_out_dir = model_out_dir,
                         raw_data = trials_frate,
                         study = "s22fu",
                         n_t=52,
                         chains = 3)
```

```{r}
fsml_compare(ratings_arl_noQ,ratings_arl_noQ_pe)
```
```{r}
filt_sum(ratings_arl_noQ$sum,"mu")
```

Fits much better.
 
# Does a model that assumes exponentially decaying effects fit better?
```{r}
fsml_compare(combined_shrink_exdec,combined_shrink)
```
Yes

# Get parameter estimates from final model not reported in the main article
```{r}
unreported_params <- "phi_mu|base_mu|blk_num_mu|dec_mu|tr_mu|pr_mu|alpha_mu|tau_mu|forget_mu"
up_tab <- select(filt_sum(model_pred_arl_nonuis$sum,unreported_params),variable,median,q5,q95)
up_tab[2,3,4] <- as.numeric(up_tab[2,3,4])
```

```{r}
#replace the choice predictors with scaled effects
scd_csens_mu <- get_draws("model_pred_arl_nonuis",vars=c("scd_phi_mu"),model_out_dir=model_out_dir)
up_tab[2,2:4] <- t(quantile(scd_csens_mu,c(.50,.05,.95)))
```

```{r}
#sigmoid-transform the learning rates
up_tab[c(1,3,4),2:4] <- sigmoid(up_tab[c(1,3,4),2:4])
```

```{r}
up_tab[,c("median","q5","q95")] <- round(up_tab[,c("median","q5","q95")],2)
write.csv(up_tab,"~/Desktop/temp.csv")
```

# Testing for spurious positive effects of reward associations
Here, we consider whether the observed effect of reward associations on choice could be due to its collinearity
with affect assocations.

First, we fit a model and simulate data in which only affect associations influence choice
```{r}
aff_only_s2 <- fit_stan_model(stan_file = paste0(stan_model_dir_s22fu,"aff_only_s2.stan"),
                         model_out_dir = model_out_dir,
                         raw_data = trials,
                         study = "s22fu",
                         iter_warmup = 10,
                         iter_sampling = 10,
                         n_t=104)
aff_only_s2 <- read_fsml("aff_only_s2",model_out_dir=model_out_dir)
```


For each simulated dataset, we estimate the effects of Q and A on choice, using an analysis method that 
efficiently approximates the model reported in the manuscript
```{r}
# Get necessary data
trials_bqba_sim <- trials %>% select(sub_index,block,fA_ix,fB_ix,feed_probe_number,out_a,out_b)

# Add parameter means needed for approximate data analysis
trials_bqba_sim <- add_param_means(aff_only_s2$sum,"alpha",trials_bqba_sim)
trials_bqba_sim <- add_param_means(aff_only_s2$sum,"forget",trials_bqba_sim)
trials_bqba_sim <- add_param_means(aff_only_s2$sum,"tau",trials_bqba_sim)

# Get simulated data
sim_choice <- get_draws(model="aff_only_s2",vars=c("sim_choice"),model_out_dir=model_out_dir)
sim_feed_rat <- get_draws("aff_only_s2",vars=c("sim_feed_rat"),model_out_dir=model_out_dir)
sim_prat <- get_draws("aff_only_s2",vars=c("sim_prat"),model_out_dir=model_out_dir)

plan(multisession, workers = 12)
registerDoFuture()

sim_effs <- data.frame("bQs"=c(),"bAs"=c())
for(chunk in 1:8){
  # Divide the simulations into 8 chunks, to avoid overtaxing memory
  iters <- ((chunk-1)*125+1):(chunk*125)
  sim_choice_chunk <- sim_choice[iters,,]
  sim_prat_chunk <- sim_prat[iters,,]
  sim_feed_rat_chunk <- sim_feed_rat[iters,,]
  chunk_effs <- foreach(i = 1:125, .combine = rbind) %dopar% {
    iter_effs <- data.frame(matrix(0,ncol=2,nrow=4))
    colnames(iter_effs) <- c("bQs","bAs")
    for(c in 1:4){
      # Add simulated choices and ratings to the data
      sim_trials <- trials_bqba_sim %>% 
                      mutate(choice=as.vector(sim_choice_chunk[i,c,])) %>%
                      mutate(prat=as.vector(sim_prat_chunk[i,c,])) %>%
                      left_join(data.frame(feed_rate_z = as.vector(sim_feed_rat_chunk[i,c,]),
                                           feed_probe_number = 1:dim(sim_feed_rat_chunk)[3]),
                                by="feed_probe_number") %>%
                      mutate(chosen_frac = ifelse(choice == 1,fA_ix,fB_ix)) %>%
                      mutate(unchosen_frac = ifelse(choice == 1,fB_ix,fA_ix))
      # Get estimated effects of Q and A on choice
      est_eff <- est_bq_ba_s2(sim_trials)
      iter_effs$bQs[c] <- est_eff$bQ
      iter_effs$bAs[c] <- est_eff$bA
    }
    return(iter_effs)
  }
  sim_effs <- rbind(sim_effs,chunk_effs)
}
```



