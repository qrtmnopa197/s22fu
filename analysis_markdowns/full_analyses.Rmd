---
title: "Full analysis"
author: "Daniel Parr"
knit: (function(inputFile, encoding) {
  rmarkdown::render(inputFile, encoding = encoding, output_dir = "/Users/dp/projects/s22_follow_up/data_review_analysis_records") })
output: html_document
---
# Overview
This markdown contains the full set of analyses done on the s22 follow-up study.

# Setting up
Loading in functions and data; setting paths and options.
```{r 1, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(mc.cores=12)

##SET MANUALLY
path_to_project_directory <- "~/projects/s22_follow_up/"
path_to_s22 <- "~/projects/spring_2022_study/"
##############
stan_model_dir_s22fu <- paste0(path_to_project_directory,"code/stan_models/")
stan_model_dir_s22 <- paste0(path_to_s22,"code/stan_models/final_models/")
model_out_dir <- paste0(path_to_project_directory,"output/results/stan_model_fits/")

library(cmdstanr)
library(tidyverse)
library(tidybayes)
library(loo)
library(GGally)
library(bayesplot)
library(sigmoid)
library(abind)

source(paste0(path_to_project_directory,"code/functions/s22fu_utilities.R"))
source(paste0(path_to_s22,"code/functions/s22_utilities.R"))
source(paste0(path_to_s22,"code/functions/stan_utilities.R"))
source(paste0(path_to_s22,"code/functions/fit_stan_model.R"))

trials <- read.csv(paste0(path_to_project_directory,"analysis_data/trial_level_data_all_subs_2023-06-12_09_56_03.csv"))
subs <- read.csv(paste0(path_to_project_directory,"analysis_data/sub_level_data_all_subs_2023-06-12_09_56_03.csv"))
```

# Pre-processing
Eliminate subject who fail to meet quality standards

```{r 2}
#identify subjects who fail the hard QC cutoffs
sub_hard_fail <- subs %>% filter(att_checks_passed < 3 |
                            percent_left > .8 |
                            percent_left < .2 |
                            consecutive_late_choices > 5 |
                            late_percent > .2 |
                            answers_incorrect > 2 |
                            trials_completed < 104 |
                            valrate_skipped_percent > .14 |
                            valence_sd < .1 |
                            decrate_sd < .05 |
                            feedrate_sd < .05)

#count the number of soft QC cutoffs each subject meets
subs$softs <- 0
for(i in 1:nrow(subs)){
  subs$softs[i] <- length(which(c(subs$consecutive_auto_process[i] > 4,subs$att_checks_passed[i] < 4,subs$answers_incorrect[i] > 1,
                                  subs$late_percent[i] > .1,subs$valrate_skipped_percent[i] > .07,
                                  subs$choice_pt_completed[i] == 0, subs$decrate_pt_completed[i] == 0, subs$feedrate_pt_completed == 0)))
}
sub_soft_fail <- filter(subs,softs >= 2) #identify those who meet more than 2 soft cutoffs

subs_to_exclude <- unique(c(sub_hard_fail$id,sub_soft_fail$id)) #mark subjects who fail on either hard or soft or criteria for exclusion

trials <- trials %>% filter(!(id %in% subs_to_exclude)) %>% filter(choice != "late") #filter out bad subjects, as well as trials on which the subject failed to make a choice
subs <- subs %>% filter(!(id %in% subs_to_exclude))
```

Do a few data transformations.
```{r 3}
trials <- add_sub_indices(trials) #add subject indices to the df. These will match the indices used in Stan.

#add a column with completed trial numbers - the trial indices if you ignore late trials. These will match the "t" trial numbers used in the Stan models
trials <- do.call(rbind,by(trials,trials$sub_index,add_trials_nl))
trials$overall_trial_nl <- 1:nrow(trials) #get the overall trial number ignoring late trials and collapsing across subjects

#get mean-centered trial and block predictors for easier fitting in Stan
trials$trial_nl_cent <- trials$trial_nl - mean(trials$trial_nl)                                                          
trials$block_cent <- trials$block - mean(trials$block) 

#Create indices from 1:n_f for each fractal image. To do this, first create a mini-df with one column having all the fA_img values and the other two
#columns having indices for fA and fB. This assumes that every fA_img is paired with a unique fB_img.
f_index_df <- data.frame(fA_img = unique(trials$fA_img),fA_ix = 1:length(unique(trials$fA_img)),fB_ix = (1:length(unique(trials$fA_img))+length(unique(trials$fA_img))))
trials <- left_join(trials,f_index_df,by="fA_img")

#get the chosen fractal index
trials <- trials %>% mutate(chosen_frac = ifelse(choice == "fA",fA_ix,fB_ix))
trials <- trials %>% mutate(unchosen_frac = ifelse(choice == "fA",fB_ix,fA_ix))

#add a column with the affect probe number for each subject (999 if no probe response). These will be passed into Stan
trials <- add_probe_number(trials,newcol="dec_probe_number",val_col="dec_rate") #for decision probes
trials <- add_probe_number(trials,newcol="feed_probe_number",val_col="feed_rate") #for decision probes
```

# Data review

```{r}
percentage_plots(trials,"stay")
```

```{r}
choice_fit <- lmer(choice_numeric ~ fB_win_prob + fA_win_prob + (1|id),trials)
summary(choice_fit)
```

```{r}
stay_fit <- lmer(stay ~ chosen_out + unchosen_out + (1|id),trials)
summary(stay_fit)
```

```{r}
ages <- subs$age[subs$age != 999]
mean(ages)
sd(ages)
```

```{r}
sum(subs$gender=="Male")/sum(subs$gender!=999)
```

```{r}
sum(subs$gender=="Female")/sum(subs$gender!=999)
```

```{r}
sum(subs$gender=="Non-binary")/sum(subs$gender!=999)
```

# Choice models

Fitting Q-learning models to choice.

## Raw Q-learning

Fitting a basic Q-learning model with no nuisance parameters.
```{r 5}
basic_qlearn <- fit_stan_model(stan_file = paste0(stan_model_dir_s22,"basic_qlearn.stan"),
                         model_out_dir = model_out_dir,
                         raw_data = trials,
                         study = "s22fu",
                         skip=c("waic","check_csvs"),
                         n_t=104)
```

## Q-learning with autocorrelation

Now adding an autocorrelation component to the Q-learning model.
```{r}
autocor_qlearn <- fit_stan_model(stan_file = paste0(stan_model_dir_s22,"autocor_qlearn.stan"),
                         model_out_dir = model_out_dir,
                         raw_data = trials,
                         study = "s22fu",
                         skip=c("waic","check_csvs"),
                         n_t=104)
```

```{r}
fsml_compare(basic_qlearn,autocor_qlearn)
```
Autocorrelation model fits better.

## Q-learning with autocorrelation and side bias

Adding left-side bias as a parameter that influence choices.
```{r}
autocor_qlearn_sidebias <- fit_stan_model(stan_file = paste0(stan_model_dir_s22fu,"autocor_qlearn_sidebias.stan"),
                         model_out_dir = model_out_dir,
                         raw_data = trials,
                         study = "s22fu",
                         skip=c("waic","check_csvs"),
                         n_t=104)
```

```{r}
fsml_compare(autocor_qlearn,autocor_qlearn_sidebias)
```
Not sufficient evidence that the sidebias model fits better.

## Q-learning with autocorrelation and forgetting, no independent decay parameter

Update Q values of fractals not presented toward 0
```{r}
autocor_alphforget_qlearn <- fit_stan_model(stan_file = paste0(stan_model_dir_s22,"autocor_alphforget_qlearn.stan"),
                         model_out_dir = model_out_dir,
                         raw_data = trials,
                         study = "s22fu",
                         skip=c("waic","check_csvs"),
                         n_t=104)
```

```{r}
fsml_compare(autocor_qlearn,autocor_alphforget_qlearn)
```
Not sufficient evidence that this fits better.

## Q-learning with autocorrelation and forgetting, independent decay parameter

Update Q values of fractals not presented toward 0, using an independent learning rate for this process
```{r}
autocor_hierforget_qlearn <- fit_stan_model(stan_file = paste0(stan_model_dir_s22,"autocor_hierforget_qlearn.stan"),
                         model_out_dir = model_out_dir,
                         raw_data = trials,
                         study = "s22fu",
                         skip=c("waic","check_csvs"),
                         n_t=104)
```

```{r}
fsml_compare(autocor_qlearn,autocor_alphforget_qlearn,autocor_hierforget_qlearn)
```
The model with an independent decay parameter fits best.

## Winning choice model review

Verifying that the effect of reward on choice is consistently positive by looking at the subject-level mean beta values.
```{r}
ncp_mean_hist(autocor_hierforget_qlearn$sum,"beta")
```
Yes, it is.

Looking at the mean effects of each parameter.
```{r}
filt_sum(autocor_hierforget_qlearn$sum,"mu")
```

# Affect models

## Model comparison

First, fitting models that include variables only for one theory. These fill be fit without shrinkage priors, since they include only 2-3 predictors of interest each.

Simple value model
```{r}
simp_val_noshrink <- fit_stan_model(stan_file = paste0(stan_model_dir_s22fu,"simp_val_noshrink.stan"),
                                    model_out_dir = model_out_dir,
                                    raw_data = trials,
                                    study = "s22fu",
                                    skip=c("waic","check_csvs"),
                                    n_t=104)
```

PE model
```{r}
pe_noshrink <- fit_stan_model(stan_file = paste0(stan_model_dir_s22fu,"pe_noshrink.stan"),
                                    model_out_dir = model_out_dir,
                                    raw_data = trials,
                                    study = "s22fu",
                                    iter_warmup = 4000,
                                    iter_sampling = 4000,
                                    adapt_delta = .95,
                                    max_treedepth = 12,
                                    skip=c("waic","check_csvs"),
                                    n_t=104)
```

Regret model
```{r}
regret_noshrink <- fit_stan_model(stan_file = paste0(stan_model_dir_s22fu,"regret_noshrink.stan"),
                                    model_out_dir = model_out_dir,
                                    raw_data = trials,
                                    study = "s22fu",
                                    skip=c("waic","check_csvs"),
                                    n_t=104)
```


Fitting the combined model (without a shrinkage prior, for greater comparability).
```{r}
combined_noshrink <- fit_stan_model(stan_file = paste0(stan_model_dir_s22fu,"combined_noshrink.stan"),
                                    model_out_dir = model_out_dir,
                                    raw_data = trials,
                                    study = "s22fu",
                                    skip=c("waic","check_csvs"),
                                    n_t=104)
```

Finally, fitting a null model (just nuisance parameters, no predictors of interest).
```{r}
null_affect <- fit_stan_model(stan_file = paste0(stan_model_dir_s22fu,"null_affect.stan"),
                         model_out_dir = model_out_dir,
                         raw_data = trials,
                         study = "s22fu",
                         skip=c("waic","check_csvs"),
                         n_t=104)
```

Now, comparing each theory-specific model to the null
```{r}
fsml_compare(simp_val_noshrink,null_affect)
fsml_compare(regret_noshrink,null_affect)
fsml_compare(pe_noshrink,null_affect)
```


Comparing each theory-specific model to the combined model.
```{r}
fsml_compare(combined_noshrink,pe_noshrink,regret_noshrink,simp_val_noshrink)
```

## Posterior effect estimates

### Main analysis

Looking at the effect estimates for each of the individual variables.

First, refitting the combined model with a shrinkage prior.
```{r}
combined_shrink <- fit_stan_model(stan_file = paste0(stan_model_dir_s22fu,"combined_shrink.stan"),
                         model_out_dir = model_out_dir,
                         raw_data = trials,
                         study = "s22fu",
                         skip=c("waic","check_csvs"),
                         n_t=104)
```

Calculating R-squared for this model
```{r}
dec_rsq <- mcmc_rsq(combined_shrink$fit,"dec_pred","d_resid",print_plot=FALSE)
dec_rsq$sum
```

```{r}
feed_rsq <- mcmc_rsq(combined_shrink$fit,"feed_pred","f_resid",print_plot=FALSE)
feed_rsq$sum
```

Now, examining 90% credible intervals for the effects
```{r}
cs_mudraws <- get_draws_df(combined_shrink$mod,combined_shrink$fit,vars=c("dpiw_mu[1]","dpiw_mu[2]","dpiw_mu[3]","fpiw_mu[1]","fpiw_mu[2]","fpiw_mu[3]","fpiw_mu[4]"))
cs_mudraws_origform <- cs_mudraws %>% transmute(choice_TDE = -`dpiw_mu[1]`, Q_ch = `dpiw_mu[1]` + `dpiw_mu[2]` + `dpiw_mu[3]`, Q_regret=`dpiw_mu[3]`, 
                                                      PWRD = -`fpiw_mu[1]`,RPE = -`fpiw_mu[2]`,reward = `fpiw_mu[1]` + `fpiw_mu[2]` + `fpiw_mu[3]` + `fpiw_mu[4]`, regret = `fpiw_mu[4]`)
mcmc_areas(
  cs_mudraws_origform,
  area_method = "scaled height",
  prob = 0.5, # 80% intervals
  prob_outer = 0.9, # 99%
  point_est = "mean"
)
```
Because two of the prediction error effects are in the wrong direction, it raises the question of whether the combined model still fits better without these prediction-inconsistent effects.
Refitting the model with no effects of TDE or RPE.
```{r}
combined_noshrink_noVQf <- fit_stan_model(stan_file = paste0(stan_model_dir_s22fu,"combined_noshrink_noVQf.stan"),
                         model_out_dir = model_out_dir,
                         raw_data = trials,
                         study = "s22fu",
                         skip=c("waic","check_csvs"),
                         n_t=104)
```

```{r}
fsml_compare(combined_noshrink_noVQf,pe_noshrink,regret_noshrink,simp_val_noshrink)
```
Yes, the combined model is still clearly superior.

### Exponential decay
How does the fit look when we assume exponentially decaying effects, as in Study 1?
```{r}
combined_shrink_exdec <- fit_stan_model(stan_file = paste0(stan_model_dir_s22fu,"combined_shrink_exdec.stan"),
                         model_out_dir = model_out_dir,
                         raw_data = trials,
                         study = "s22fu",
                         skip=c("waic","check_csvs"),
                         n_t=104)
```

```{r}
fsml_compare(combined_shrink,combined_shrink_exdec)
```

Not doing exponential decay works best

### Supplementary effect estimates

Estimating these effects a few different ways as a robustness check on the above estimates, and for desriptive purposes.

Looking at the effect estimates for the raw valence predictors
```{r}
mcmc_areas(
  cs_mudraws,
  area_method = "scaled height",
  prob = 0.5, # 80% intervals
  prob_outer = 0.9, # 99%
  point_est = "mean"
)
```

Running the combined model with no nuisance parameters added to the choice model
```{r}
combined_shrink_rawQ <- fit_stan_model(stan_file = paste0(stan_model_dir_s22fu,"combined_shrink_rawQ.stan"),
                         model_out_dir = model_out_dir,
                         raw_data = trials,
                         study = "s22fu",
                         skip=c("waic","check_csvs"),
                         n_t=104)
```

Plotting effects
```{r}
csr_mudraws <- get_draws_df(combined_shrink_rawQ$mod,combined_shrink_rawQ$fit,vars=c("dpiw_mu[1]","dpiw_mu[2]","dpiw_mu[3]","fpiw_mu[1]","fpiw_mu[2]","fpiw_mu[3]","fpiw_mu[4]"))

csr_mudraws_origform <- csr_mudraws %>% transmute(choice_TDE = -`dpiw_mu[1]`, Q_ch = `dpiw_mu[1]` + `dpiw_mu[2]` + `dpiw_mu[3]`, Q_regret=`dpiw_mu[3]`, 
                                                      PWRD = -`fpiw_mu[1]`,RPE = -`fpiw_mu[2]`,reward = `fpiw_mu[1]` + `fpiw_mu[2]` + `fpiw_mu[3]` + `fpiw_mu[4]`, regret = `fpiw_mu[4]`)
mcmc_areas(
  csr_mudraws_origform,
  area_method = "scaled height",
  prob = 0.5, # 80% intervals
  prob_outer = 0.9, # 99%
  point_est = "mean"
)
```

Now, examining effects from each theory-specific model.

Simple value model
```{r}
csr_mudraws <- get_draws_df(combined_shrink_rawQ$mod,combined_shrink_rawQ$fit,vars=c("dpiw_mu[1]","dpiw_mu[2]","dpiw_mu[3]","fpiw_mu[1]","fpiw_mu[2]","fpiw_mu[3]","fpiw_mu[4]"))

csr_mudraws_origform <- csr_mudraws %>% transmute(choice_TDE = -`dpiw_mu[1]`, Q_ch = `dpiw_mu[1]` + `dpiw_mu[2]` + `dpiw_mu[3]`, Q_regret=`dpiw_mu[3]`, 
                                                      PWRD = -`fpiw_mu[1]`,RPE = -`fpiw_mu[2]`,reward = `fpiw_mu[1]` + `fpiw_mu[2]` + `fpiw_mu[3]` + `fpiw_mu[4]`, regret = `fpiw_mu[4]`)
sv_draws <- simp_val_noshrink$fit$draws(c("dpiw_mu","fpiw_mu"))
mcmc_areas(
  sv_draws,
  area_method = "scaled height",
  prob = 0.5, # 80% intervals
  prob_outer = 0.9, # 99%
  point_est = "mean"
)
```

PE model
```{r}
pe_draws <- pe_noshrink$fit$draws(c("dpiw_mu","fpiw_mu"))
temp <- mcmc_areas(
  pe_draws,
  area_method = "scaled height",
  prob = 0.5, # 80% intervals
  prob_outer = 0.9, # 99%
  point_est = "mean"
  )
```

Regret model
```{r}
regret_draws <- regret_noshrink$fit$draws(c("dpiw_mu","fpiw_mu"))
mcmc_areas(
  regret_draws,
  area_method = "scaled height",
  prob = 0.5, # 80% intervals
  prob_outer = 0.9, # 99%
  point_est = "mean"
)
```
# Linear algebra reanalysis

```{r}
#Create direction vectors, where the dimensions are int he order:
#       Vb,Qcd,Qud,Vt,Qcf,rc,ru
tde <- c(-.5,.5,0,0,0,0,0)
q_ch <- c(0,1,0,0,0,0,0)
q_reg <- c(0,.5,-.5,0,0,0,0)
pwrd <- c(0,0,0,-.5,0,.5,0)
rpe <- c(0,0,0,0,-.5,.5,0)
rew <- c(0,0,0,0,0,1,0)
out_reg <- c(0,0,0,0,0,.5,-.5)
dvec_mat <- cbind(tde,q_ch,q_reg,pwrd,rpe,rew,out_reg) #turn into matrix

combined_shrink_fit <- load_cmdstan_fit(model_out_dir=model_out_dir,"combined_shrink")
cs_draws <- combined_shrink_fit$draws(c("dpiw_mu","fpiw_mu")) #get draws array
#vec_ws <- apply(cs_draws,c(1,2),vec_optim,dvec=dvec_mat) #get vector weights
#vec_ws_org <- aperm(vec_ws,c(2,3,1)) #rearrange to be in the same order that they were in the draws array
#dimnames(vec_ws_org)[[3]] <- c("tde","q_ch","q_reg","pwrd","rpe","rew","out_reg") #name meaningfully
#save(vec_ws_org,file=paste0(model_out_dir,"vec_ws_s2.RData"))
load(paste0(model_out_dir,"vec_ws_s2.RData"))
```

```{r}
#plot raw weights, in the order Q_ch,choice; V_block; Q_unch; r_ch;, Q_ch,out; V_trial; r_unch
raw_eff_intervals<- create_interval_plot(arr = cs_draws,names = c("fpiw_mu[4]","fpiw_mu[3]","fpiw_mu[2]","fpiw_mu[1]","dpiw_mu[3]","dpiw_mu[2]","dpiw_mu[1]"),
                                         color_mapping = c("fpiw_mu[1]" = "#414141","fpiw_mu[2]" = "#414141","fpiw_mu[3]" = "#414141","fpiw_mu[4]" = "#414141",
                                                           "dpiw_mu[1]" = "#414141","dpiw_mu[2]" = "#414141","dpiw_mu[3]" = "#414141",
                                                           "fpiw_mu[1]_med" = "black","fpiw_mu[2]_med" = "black","fpiw_mu[3]_med" = "black","fpiw_mu[4]_med" = "black",
                                                           "dpiw_mu[1]_med" = "black","dpiw_mu[2]_med" = "black","dpiw_mu[3]_med" = "black"),
                                          xmin = -.2,xmax = .85,
                                          percentiles = c(0.025,.25,.50,.75,.975),
                                          dot_size = 3)
raw_eff_intervals_s2 <- raw_eff_intervals
save(raw_eff_intervals_s2,file="~/Documents/active_manuscript/manuscript/figures/raw_eff_intervals_s2.RData")
```


```{r}
#plot normed weights
mn_data <- apply(cs_draws,c(1,2),function(x) sum(abs(x))) #get the manhattan norms for each data vector by summing the absolute values

comb_array <- abind(vec_ws_org,mn_data,along=3) #staple mn_data to the back of the third dimension of the vector weight array
vec_ws_norm <- apply(comb_array,c(1,2), get_ports) #get portions of relationship accounted for

#have to rearrange and name again
vec_ws_norm_org <- aperm(vec_ws_norm,c(2,3,1)) 
dimnames(vec_ws_norm_org)[[3]] <- c("tde","q_ch","q_reg","pwrd","rpe","rew","out_reg","resid") #name meaningfully
```

Get stats
```{r}
quantile(vec_ws_norm_org[,,"q_ch"],c(.50,.025,.975))
round(mean(vec_ws_norm_org[,,"q_ch"] > 0) * 100,1)

quantile(vec_ws_norm_org[,,"rew"],c(.50,.025,.975))
round(mean(vec_ws_norm_org[,,"rew"] > 0) * 100,1)

quantile(vec_ws_norm_org[,,"pwrd"],c(.50,.025,.975))
round(mean(vec_ws_norm_org[,,"pwrd"] > 0) * 100,1)

quantile(vec_ws_norm_org[,,"out_reg"],c(.50,.025,.975))
round(mean(vec_ws_norm_org[,,"out_reg"] > 0) * 100,1)
```

Plot intervals
```{r}
#create interval plots for manhattan norm ratios
variable_intervals_s2<- create_interval_plot(arr = vec_ws_norm_org,names = c("out_reg","pwrd","rpe","rew","q_reg","tde","q_ch"),
                                          xmin = -.012,xmax = .48) +
                        theme(panel.grid.major.x = element_line(color = "#DCDCDC", size = .47),
                              panel.background = element_rect(fill = "white", color = NA))
save(variable_intervals_s2,file="~/Documents/active_manuscripts/s22/figures/variable_intervals_s2.RData")
```


```{r}
#get sub-arrays of normed weights for each theory
sv_ws <- vec_ws_norm_org[,,c(2,6)] 
pe_ws <- vec_ws_norm_org[,,c(1,4,5)] 
reg_ws <- vec_ws_norm_org[,,c(3,7)] 

#sum them to get total theory weights
sv_ports <- apply(sv_ws,c(1,2),sum) 
pe_ports <- apply(pe_ws,c(1,2),sum)
reg_ports <- apply(reg_ws,c(1,2),sum)

thr_ports <- array(c(sv_ports,pe_ports,reg_ports,vec_ws_norm_org[,,8]),dim=c(1000,4,4))
dimnames(thr_ports)[[3]] <- c("sv","pe","cc","resid") #name meaningfully
```

Get stats for theory vectors
```{r}
quantile(thr_ports[,,"sv"],c(.50,.025,.975))
round(mean(thr_ports[,,"sv"] > 0) * 100,1)

quantile(thr_ports[,,"pe"],c(.50,.025,.975))
round(mean(thr_ports[,,"pe"] > 0) * 100,1)

quantile(thr_ports[,,"cc"],c(.50,.025,.975))
round(mean(thr_ports[,,"cc"] > 0) * 100,1)
```
Calculate differences between theory vectors, getting stats
```{r}
sv_cc <- thr_ports[,,"sv"] - thr_ports[,,"cc"]
sv_pe <- thr_ports[,,"sv"] - thr_ports[,,"pe"]
cc_pe <- thr_ports[,,"cc"] - thr_ports[,,"pe"]

quantile(cc_pe,c(.025,.975,.50))
round(sum(cc_pe > 0)/4000 * 100,1)

quantile(sv_cc,c(.025,.975,.50))
round(sum(sv_cc > 0)/4000 * 100,1)

quantile(sv_pe,c(.025,.975,.50))
round(sum(sv_pe > 0)/4000 * 100,1)
```

Plot theory vectors
```{r}
theory_intervals_s2 <- create_interval_plot(arr = thr_ports,names = c("resid","cc","pe","sv"),
                                          xmin = -.012,xmax = .8) +
                        theme(panel.grid.major.x = element_line(color = "#DCDCDC", size = 1),
                              panel.background = element_rect(fill = "white", color = NA))
save(theory_intervals_s2,file="~/Documents/active_manuscripts/s22/figures/theory_intervals_s2.RData")
```

```{r}
#Get the likelihood of each theory combination...

draw_codes <- apply(thr_ports[,,-4],c(1,2),code_draw) #code each draw according to which theories are non-zero
#get percentage likelihoods for each theory combination
paste0("Simple value, PE, regret: ",100*sum(draw_codes == "123")/length(draw_codes))
paste0("Simple value, PE: ",100*sum(draw_codes == "12")/length(draw_codes))
paste0("Simple value, regret: ",100*sum(draw_codes == "13")/length(draw_codes))
paste0("PE, regret: ",100*sum(draw_codes == "23")/length(draw_codes))
paste0("Simple value: ",100*sum(draw_codes == "1")/length(draw_codes))
paste0("PE: ",100*sum(draw_codes == "2")/length(draw_codes))
paste0("Regret: ",100*sum(draw_codes == "3")/length(draw_codes))
```
## Compare vectors across studies

Variable vectors
```{r}
# load("~/projects/spring_2022_study/output/results/stan_model_fits/final_models/vec_ws_norm_org_s1.RData") #load in Study 1 vectors
# diff_var_vecs <- vec_ws_norm_org - vec_ws_norm_org_s1
# save(diff_var_vecs,file=paste0(model_out_dir,"diff_var_vecs.RData"))
load(paste0(model_out_dir,"diff_var_vecs.RData"))


quantile(diff_var_vecs[,,"rew"],c(.025,.975,.50))
round(sum(diff_var_vecs[,,"rew"] > 0)/4000 * 100,1)

quantile(diff_var_vecs[,,"q_ch"],c(.025,.975,.50))
round(sum(diff_var_vecs[,,"q_ch"] > 0)/4000 * 100,1)

quantile(diff_var_vecs[,,"rpe"],c(.025,.975,.50))
round(sum(diff_var_vecs[,,"rpe"] < 0)/4000 * 100,1)

quantile(diff_var_vecs[,,"out_reg"],c(.025,.975,.50))
round(sum(diff_var_vecs[,,"out_reg"] < 0)/4000 * 100,1)

quantile(diff_var_vecs[,,"pwrd"],c(.025,.975,.50))
round(sum(diff_var_vecs[,,"pwrd"] < 0)/4000 * 100,1)

quantile(diff_var_vecs[,,"q_reg"],c(.025,.975,.50))
round(sum(diff_var_vecs[,,"q_reg"] < 0)/4000 * 100,1)

quantile(diff_var_vecs[,,"tde"],c(.025,.975,.50))
round(sum(diff_var_vecs[,,"tde"] < 0)/4000 * 100,1)

```

Theory vectors
```{r}
# load("~/projects/spring_2022_study/output/results/stan_model_fits/final_models/thr_ports_s1.RData")
# diff_theory_vecs <- thr_ports - thr_ports_s1
# save(diff_theory_vecs,file=paste0(model_out_dir,"diff_theory_vecs.RData"))
load(paste0(model_out_dir,"diff_theory_vecs.RData"))
quantile(diff_theory_vecs[,,"Simple value"],c(.025,.975,.50))
round(sum(diff_theory_vecs[,,"Simple value"] > 0)/4000 * 100,1)

quantile(diff_theory_vecs[,,"Regret"],c(.025,.975,.50))
round(sum(diff_theory_vecs[,,"Regret"] < 0)/4000 * 100,1)

quantile(diff_theory_vecs[,,"PE"],c(.025,.975,.50))
round(sum(diff_theory_vecs[,,"PE"] < 0)/4000 * 100,1)
```
# Plot fit vectors

Create a plot of the expected effect vector, the best approximation to this vector, and uncertainties in variable vector lengths and the effect vector.
```{r}
eff_mean <- apply(cs_draws,3,mean) #get mean effect vector

#approximate this vector
mean_vec_ws <- vec_optim(eff_mean,dvec_mat)
mean_vecs <- matrix(NA,nrow=nrow(dvec_mat),ncol=ncol(dvec_mat))
for(i in 1:ncol(mean_vecs)){
  mean_vecs[,i] <- mean_vec_ws[i]*dvec_mat[,i]
}
resid_mean <- eff_mean - rowSums(mean_vecs)


#make the order PE, CC, SV, with RPE before PWRD
mean_vecs <- mean_vecs[,c(1,3,2,5,4,7,6)]

vec_theory <- c("PE","CC","SV","PE","PE","CC","SV") #theories to which the vectors belong
                              
vec_sol_ch1 <- vec_sol_plot(dimx=1,dimy=2,vec_theory,mean_vecs,eff_mean,resid_mean,eff_draws=cs_draws,ci=0.95)
vec_sol_ch2 <- vec_sol_plot(dimx=3,dimy=2,vec_theory,mean_vecs,eff_mean,resid_mean,eff_draws=cs_draws,ci=0.95,add_resid=F)
vec_sol_out1 <- vec_sol_plot(dimx=5,dimy=6,vec_theory,mean_vecs,eff_mean,resid_mean,eff_draws=cs_draws,ci=0.95)
vec_sol_out2 <- vec_sol_plot(dimx=4,dimy=7,vec_theory,mean_vecs,eff_mean,resid_mean,eff_draws=cs_draws,ci=0.95)
             

ggsave("~/Documents/active_manuscripts/s22/figures/vec_sol_ch1_fu.pdf",vec_sol_ch1,width=2,height=2)
ggsave("~/Documents/active_manuscripts/s22/figures/vec_sol_ch2_fu.pdf",vec_sol_ch2,width=2,height=2)
ggsave("~/Documents/active_manuscripts/s22/figures/vec_sol_out1_fu.pdf",vec_sol_out1,width=2,height=2)
ggsave("~/Documents/active_manuscripts/s22/figures/vec_sol_out2_fu.pdf",vec_sol_out2,width=2,height=2)
```

# Do linear algebra analysis on model with raw Q values
```{r}
#Create direction vectors, where the dimensions are int he order:
#       Vb,Qcd,Qud,Vt,Qcf,rc,ru
tde <- c(-.5,.5,0,0,0,0,0)
q_ch <- c(0,1,0,0,0,0,0)
q_reg <- c(0,.5,-.5,0,0,0,0)
pwrd <- c(0,0,0,-.5,0,.5,0)
rpe <- c(0,0,0,0,-.5,.5,0)
rew <- c(0,0,0,0,0,1,0)
out_reg <- c(0,0,0,0,0,.5,-.5)
dvec_mat <- cbind(tde,q_ch,q_reg,pwrd,rpe,rew,out_reg) #turn into matrix

combined_shrink_rawQ_fit <- load_cmdstan_fit(model_out_dir=model_out_dir,"combined_shrink_rawQ")
csr_draws <- combined_shrink_rawQ_fit$draws(c("dpiw_mu","fpiw_mu")) #get draws array

vec_ws_rq <- apply(csr_draws,c(1,2),vec_optim,dvec=dvec_mat) #get vector weights
vec_ws_org_rq <- aperm(vec_ws_rq,c(2,3,1)) #rearrange to be in the same order that they were in the draws array
dimnames(vec_ws_org_rq)[[3]] <- c("tde","q_ch","q_reg","pwrd","rpe","rew","out_reg") #name meaningfully
#save(vec_ws_org_rq,file=paste0(model_out_dir,"vec_ws_rq_s2.RData"))
load(paste0(model_out_dir,"vec_ws_s2.RData"))
```


```{r}
#plot normed weights
mn_data_rq<- apply(csr_draws,c(1,2),function(x) sum(abs(x))) #get the manhattan norms for each data vector by summing the absolute values

comb_array_rq <- abind(vec_ws_org_rq,mn_data_rq,along=3) #staple mn_data to the back of the third dimension of the vector weight array
vec_ws_norm_rq <- apply(comb_array_rq,c(1,2), get_ports) #get portions of relationship accounted for

#have to rearrange and name again
vec_ws_norm_org_rq <- aperm(vec_ws_norm_rq,c(2,3,1)) 
dimnames(vec_ws_norm_org_rq)[[3]] <- c("tde","q_ch","q_reg","pwrd","rpe","rew","out_reg","resid") #name meaningfully
```

Plot intervals
```{r}
#create interval plots for manhattan norm ratios
variable_intervals_s2<- create_interval_plot(arr = vec_ws_norm_org_rq,names = c("out_reg","pwrd","rpe","rew","q_reg","tde","q_ch"),
                                          xmin = -.012,xmax = .48)
save(variable_intervals_s2,file="~/Documents/active_manuscript/manuscript/figures/variable_intervals_s2.RData")
```


```{r}
#get sub-arrays of normed weights for each theory
sv_ws_rq <- vec_ws_norm_org_rq[,,c(2,6)] 
pe_ws_rq <- vec_ws_norm_org_rq[,,c(1,4,5)] 
reg_ws_rq <- vec_ws_norm_org_rq[,,c(3,7)] 

#sum them to get total theory weights
sv_ports_rq <- apply(sv_ws_rq,c(1,2),sum) 
pe_ports_rq <- apply(pe_ws_rq,c(1,2),sum)
reg_ports_rq <- apply(reg_ws_rq,c(1,2),sum)

thr_ports_rq <- array(c(sv_ports_rq,pe_ports_rq,reg_ports_rq,vec_ws_norm_org_rq[,,8]),dim=c(1000,4,4))
dimnames(thr_ports_rq)[[3]] <- c("sv","pe","cc","resid") #name meaningfully
```

Plot theory vectors
```{r}
theory_intervals_s2 <- create_interval_plot(arr = thr_ports_rq,names = c("resid","cc","pe","sv"),
                                          xmin = -.012,xmax = .8)
save(theory_intervals_s2,file="~/Documents/active_manuscript/manuscript/figures/theory_intervals_s2.RData")
```

```{r}
#Get the likelihood of each theory combination...

draw_codes <- apply(thr_ports[,,-4],c(1,2),code_draw) #code each draw according to which theories are non-zero
#get percentage likelihoods for each theory combination
paste0("Simple value, PE, regret: ",100*sum(draw_codes == "123")/length(draw_codes))
paste0("Simple value, PE: ",100*sum(draw_codes == "12")/length(draw_codes))
paste0("Simple value, regret: ",100*sum(draw_codes == "13")/length(draw_codes))
paste0("PE, regret: ",100*sum(draw_codes == "23")/length(draw_codes))
paste0("Simple value: ",100*sum(draw_codes == "1")/length(draw_codes))
paste0("PE: ",100*sum(draw_codes == "2")/length(draw_codes))
paste0("Regret: ",100*sum(draw_codes == "3")/length(draw_codes))
```

# ARL analyses

First, fit a model to valence ratings (at feedback)
```{r}
trials_frate <- trials %>% filter(block_feedrate == 1) #get only trials from feedback rating blocks

ratings_arl <- fit_stan_model(stan_file = paste0(stan_model_dir_s22fu,"ratings_arl.stan"),
                         model_out_dir = model_out_dir,
                         raw_data = trials_frate,
                         study = "s22fu",
                         n_t=52,
                         iter_sampling = 2000)

ra_scd_draws <- get_draws("ratings_arl",model_out_dir=model_out_dir,vars=c("scd_beta_mu","scd_aff_sens_mu"))

quantile(ra_scd_draws[,,"scd_beta_mu"],c(.05,.50,.95))
mean(ra_scd_draws[,,"scd_beta_mu"] > 0)
quantile(ra_scd_draws[,,"scd_aff_sens_mu"],c(.05,.50,.95))
mean(ra_scd_draws[,,"scd_aff_sens_mu"] > 0)

ra_asens_draws <- ratings_arl_fit$draws("aff_sens_mu")
mcmc_areas(
  ra_asens_draws,
  area_method = "scaled height",
  prob = 0.9,
  prob_outer = 0.99,
  point_est = "median"
) +
  coord_cartesian(xlim=c(0,NA))

ra_beta_draws <- ratings_arl_fit$draws("beta_mu")
mcmc_areas(
  ra_beta_draws,
  area_method = "scaled height",
  prob = 0.9,
  prob_outer = 0.99,
  point_est = "median"
) +
  coord_cartesian(xlim=c(0,NA))
```

Next, to model-predicted valence ratings (at feedback)
```{r}
model_pred_arl <- fit_stan_model(stan_file = paste0(stan_model_dir_s22fu,"model_pred_arl.stan"),
                         model_out_dir = model_out_dir,
                         raw_data = trials,
                         study = "s22fu",
                         n_t=104,
                         chains = 3)

mpa_asens_draws <- model_pred_arl$fit$draws("aff_sens_mu")
mcmc_areas(
  mpa_asens_draws,
  area_method = "scaled height",
  prob = 0.9,
  prob_outer = 0.99,
  point_est = "median"
) +
  coord_cartesian(xlim=c(0,NA))

mpa_beta_draws <- model_pred_arl$fit$draws("beta_mu")
mcmc_areas(
  mpa_beta_draws,
  area_method = "scaled height",
  prob = 0.9,
  prob_outer = 0.99,
  point_est = "median"
) +
  coord_cartesian(xlim=c(0,NA))
```

# Compare reward and affect effects

In ratings model
```{r}
ratings_arl_vars <- fit_stan_model(stan_file = paste0(stan_model_dir_s22fu,"ratings_arl_vars.stan"),
                         model_out_dir = model_out_dir,
                         raw_data = trials_frate,
                         study = "s22fu",
                         n_t=52,
                         chains = 3)
```

```{r}
rav_draws <- get_draws("ratings_arl_vars",model_out_dir=model_out_dir,vars=c("cp_var_mu","cp_var_noA_mu","cp_var_noQ_mu","cp_var_noC_mu"))
rav_draws_diff<- (rav_draws[,,"cp_var_mu"]-rav_draws[,,"cp_var_noQ_mu"]) - (rav_draws[,,"cp_var_mu"]-rav_draws[,,"cp_var_noA_mu"])
quantile(rav_draws_diff,probs=c(.01,.025,.05,.5,.95,.975,.99))
```

In model-predicted valence model
```{r}
model_pred_arl_vars <- fit_stan_model(stan_file = paste0(stan_model_dir_s22fu,"model_pred_arl_vars.stan"),
                         model_out_dir = model_out_dir,
                         raw_data = trials,
                         study = "s22fu",
                         n_t=104,
                         chains = 3)
```

```{r}
mav_draws <- get_draws("model_pred_arl_vars",model_out_dir=model_out_dir,vars=c("cp_var_mu","cp_var_noA_mu","cp_var_noQ_mu","cp_var_noC_mu"))
mav_draws_diff<- (mav_draws[,,"cp_var_mu"]-mav_draws[,,"cp_var_noQ_mu"]) - (mav_draws[,,"cp_var_mu"]-mav_draws[,,"cp_var_noA_mu"])
quantile(mav_draws_diff,probs=c(.01,.025,.05,.5,.95,.975,.99))
```

Run model-predicted valence ARL with nuisance parameters not part of the prediction
```{r}
model_pred_arl_nonuis <- fit_stan_model(stan_file = paste0(stan_model_dir_s22fu,"model_pred_arl_nonuis.stan"),
                         model_out_dir = model_out_dir,
                         raw_data = trials,
                         study = "s22fu",
                         iter_sampling = 1500,
                         n_t=104)
```

```{r}
mpan_scds <- get_draws("model_pred_arl_nonuis",model_out_dir=model_out_dir,vars=c("scd_beta_mu","scd_aff_sens_mu"))
quantile(mpan_scds[,,"scd_beta_mu"],c(.025,.5,.975))
mean(mpan_scds[,,"scd_beta_mu"] > 0)
quantile(mpan_scds[,,"scd_aff_sens_mu"],c(.025,.5,.975))
mean(mpan_scds[,,"scd_aff_sens_mu"] > 0)
quantile(mpan_scds[,,"scd_beta_mu"] - mpan_scds[,,"scd_aff_sens_mu"],c(.025,.5,.975))
mean(mpan_scds[,,"scd_beta_mu"] > mpan_scds[,,"scd_aff_sens_mu"])

```

# Exploratory analyses

```{r}
breakdown_s2 <- fit_stan_model(stan_file = paste0(stan_model_dir_s22fu,"breakdown_s2.stan"),
                         model_out_dir = model_out_dir,
                         raw_data = trials_frate,
                         study = "s22fu",
                         n_t=52,
                         chains = 3)
```

```{r}
breakdown_s2$diagnostics
filt_sum(breakdown_s2$sum,"mu")
filt_sum(breakdown_s2$sum,"sigma")
```
Affect sensitivity parameters are out of left field. Big SDs on mean estimates - plus big sigmas - suggests that we're just not powered to detect all of those effects. 


Regressing valence and reward onto choice
```{r}
trials_frate_scale <- trials %>% filter(!is.na(feed_rate_z)) %>% mutate(feed_rate_z=scale(feed_rate_z),chosen_out=scale(chosen_out),out_diff=scale(chosen_out-unchosen_out))
stay_fit <- lmer(stay ~ feed_rate_z + out_diff + (feed_rate_z + out_diff | sub_index),trials_frate_scale)
stay_fit2 <- lmer(stay ~  chosen_out + (chosen_out | sub_index),trials_frate_scale)
anova(stay_fit,stay_fit2)
summary(stay_fit)
summary(stay_fit2)
```

# Effects of affect variables on RL

```{r}
#get posteriors for affect regressors and affect sensitivity
aff_posts <- get_draws("model_pred_arl_nonuis",model_out_dir,vars=c("fpiw_mu[1]","fpiw_mu[2]","fpiw_mu[3]","fpiw_mu[4]"))
asens_post <- get_draws("model_pred_arl_nonuis",model_out_dir,vars=c("aff_sens_mu"))

#multiply each coefficient posterior by affect sensitity to get affect-mediated RL influence
aff_ch_list <- lapply(c(1:dim(aff_posts)[3]),function(dim) drop(aff_posts[,,dim])*drop(asens_post))
aff_ch <- abind(aff_ch_list,along=3)

#add the non-affect-mediated RL influence of reward to that posterior
beta_post <- get_draws("model_pred_arl_nonuis",model_out_dir,vars=c("beta_mu")) 
aff_ch[,,3] <- aff_ch[,,3] + drop(beta_post) #add to the reward posterior the effect of reward per se



pwrd <- c(-.5,0,.5,0)
rpe <- c(0,-.5,.5,0)
rew <- c(0,0,1,0)
out_reg <- c(0,0,.5,-.5)
dvec_val_mat <- cbind(pwrd,rpe,rew,out_reg) #turn into matrix

val_vec_ws <- apply(aff_ch,c(1,2),vec_optim,dvec=dvec_val_mat,init_pars=rep(0,4)) #get vector weights
vv_ws_org <- aperm(val_vec_ws,c(2,3,1)) #rearrange dims to be in the same order that they were in the draws array
dimnames(vv_ws_org)[[3]] <- c("pwrd","rpe","rew","out_reg") #name meaningfully
```

Get array of manhattan norm ratios
```{r}
#get array of manhattan norm ratios
mn_data_vv <- apply(aff_ch,c(1,2),function(x) sum(abs(x))) #get the manhattan norms for each data vector by summing the absolute values

comb_array_vv <- abind(vv_ws_org,mn_data_vv,along=3) #staple mn_data to the back of the third dimension of the vector weight array
vv_ws_norm <- apply(comb_array_vv,c(1,2), get_ports) #get portions of relationship accounted for

#have to rearrange and name again
vv_ws_norm_org <- aperm(vv_ws_norm,c(2,3,1)) 
dimnames(vv_ws_norm_org)[[3]] <- c("pwrd","rpe","rew","out_reg","resid") #name meaningfully

#create interval plots
create_interval_plot(arr = vv_ws_norm_org,names=c("pwrd","rpe","rew","out_reg","resid"),xmin = -.012,xmax = 1)
```

```{r}
#save(vv_ws_norm_org,file=paste0(model_out_dir,"vv_ws_norm_org_s2.RData"))
load(paste0(model_out_dir,"vv_ws_norm_org_s2.RData"))

#get total theory weights
sv_vv_ports <- vv_ws_norm_org[,,3] 
reg_vv_ports <- vv_ws_norm_org[,,4] 

pe_vv_ws <- vv_ws_norm_org[,,c(1,2)] 
pe_vv_ports <- apply(pe_vv_ws,c(1,2),sum)

thr_vv_ports <- array(c(sv_vv_ports,pe_vv_ports,reg_vv_ports,vv_ws_norm_org[,,5]),dim=c(1500,4,4))
dimnames(thr_vv_ports)[[3]] <- c("sv","pe","cc","resid") #name meaningfully

thr_vv_ports_s2 <- thr_vv_ports
save(thr_vv_ports_s2,file=paste0(model_out_dir,"thr_vv_ports_s2.RData"))
```

```{r}
theory_intervals_vv <- create_interval_plot(arr = thr_vv_ports,names = c("resid","cc","pe","sv"),
                                          xmin = -.017,xmax = 1)
theory_intervals_vv_s2 <- theory_intervals_vv
save(theory_intervals_vv_s2,file="~/Documents/active_manuscripts/s22/figures/theory_intervals_vv_s2.RData")
```

make the above vectors into theory vectors
```{r}
#s1
sv_ws <- vec_ws_norm_org[,,c(2,6)] 
pe_ws <- vec_ws_norm_org[,,c(1,4,5)] 
reg_ws <- vec_ws_norm_org[,,c(3,7)] 

#sum them to get total theory weights
sv_ports <- apply(sv_ws,c(1,2),sum) 
pe_ports <- apply(pe_ws,c(1,2),sum)
reg_ports <- apply(reg_ws,c(1,2),sum)

thr_ports <- array(c(sv_ports,pe_ports,reg_ports,vec_ws_norm_org[,,8]),dim=c(1000,4,4))
dimnames(thr_ports)[[3]] <- c("sv","pe","cc","resid") #name meaningfully
```

Get stats for theory vectors
```{r}
quantile(thr_ports[,,"sv"],c(.50,.025,.975))
round(mean(thr_ports[,,"sv"] > 0) * 100,1)

quantile(thr_ports[,,"pe"],c(.50,.025,.975))
round(mean(thr_ports[,,"pe"] > 0) * 100,1)

quantile(thr_ports[,,"cc"],c(.50,.025,.975))
round(mean(thr_ports[,,"cc"] > 0) * 100,1)
```


```{r}
rl_var_free_s2 <- fit_stan_model(stan_file = paste0(stan_model_dir_s22fu,"rl_var_free_s2.stan"),
                         model_out_dir = model_out_dir,
                         raw_data = trials,
                         study = "s22fu",
                         n_t=104)
```

#Theory predictions only ARL
```{r}
arl_theor_pred_s2 <- fit_stan_model(stan_file = paste0(stan_model_dir_s22fu,"arl_theor_pred_s2.stan"),
                         model_out_dir = model_out_dir,
                         raw_data = trials,
                         study = "s22fu",
                         n_t=104)
```

```{r}
atps2_scds <- get_draws("arl_theor_pred_s2",model_out_dir=model_out_dir,vars=c("scd_beta_mu","scd_aff_sens_mu"))
quantile(atps2_scds[,,"scd_beta_mu"],c(.025,.5,.975))
mean(atps2_scds[,,"scd_beta_mu"] > 0)
quantile(atps2_scds[,,"scd_aff_sens_mu"],c(.025,.5,.975))
mean(atps2_scds[,,"scd_aff_sens_mu"] > 0)

```

#Utility vs. reinforcement test

```{r}
model_pred_arl_nonuis_pe <- fit_stan_model(stan_file = paste0(stan_model_dir_s22fu,"model_pred_arl_nonuis_pe.stan"),
                         model_out_dir = model_out_dir,
                         raw_data = trials,
                         study = "s22fu",
                         iter_sampling = 1500,
                         n_t=104)
```

